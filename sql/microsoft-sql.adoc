= Introduction

I realized, I might not have the best basics whent it comes to Microsoft SQL.
And so, this is a literal refreshing, and powering up of those skills/knowledge.

The content of these notes will be based around this course:

https://www.udemy.com/course/complete-microsoft-sql-server-database-administration-course/[Udemy Course]

But of course, the idea is to also make my own ideas, investigate even further and
note all those extra bits and pieces that (imo) give a much rounder depth of
knowledge than just limiting yourself to course content.

== Complete Microsoft SQL server Database Administration Course

=== Introuction

Two instructors. They seem to be veterans with 20+ years of experience in IT
plus reputable degrees from good unis.

_Databases are my passio_ and the _Accidental DBA_ bits lead me to believe these
are green flags of the course.

=== Course Overview

Seems we will get a good mix of theory on top of practical stuff. That's good,
since my opinion is always to know your basics/theory well enough so that you
can then apply all of that practically. We are already pretty far away from
what many tutorials and materials used to be just a "copy what I do, and don't
ask questions".

Looking forward to learning interesting concepts on top of actually applying
them. _Besides just basics_. Since, well, I like learning, but also because when
push comes to shove and I need to also help a discussion with extra knowledge
that some might not have at the moment, that's when a difference can be made.
And we can ship things faster, and **get that user need fulfilled**.

=== Introduction to a Database

We know this. But in short.

- Data = Any piece of info that is unstructured but that can mean something
in a certain context (date, age, height, name)
- Databases are a solution to manage large volumes of data easily, with a way
better framework than a spreadsheet, and taking a fraction of size.
- There are powerful concepts in databases such as `joins`, `groups` and many
others. Things that in Excel are not a thing, or trying to replicate is also
way harder.

=== What is RDBMS?

Relational Database Management System.

So, a Relational Database is but **one** type of database. It uses a **structure**
that allows us to **identify and access data** in relation to another piece of data
in the database. Often, data in a relational database is organized into **tables**.

Here's an example of how the data would be organized:

.Employees Table
[cols="1,2,2,1", options="header"]
|===
| ID | First Name | Last Name | Department
| 1  | John       | Smith     | Engineering
| 2  | Maria      | Garcia    | Marketing  
| 3  | David      | Chen      | Sales
| 4  | Sarah      | Johnson   | HR
|===

.Addresses Table
[cols="1,3,2,2", options="header"]
|===
| Employee ID | Street Address       | City       | State
| 1           | 123 Main Street      | Boston     | MA
| 2           | 456 Oak Avenue       | Chicago    | IL
| 3           | 789 Pine Road        | Houston    | TX
| 4           | 321 Elm Boulevard    | Phoenix    | AZ
|===

Now, zooming out of this SQL world. Let's go back to spreadsheets for another example,
you can in theory organize data into _tables_ right? They might look the same,
but their properties are fundamentally different. Excel tables have no inherent
relation between them, and you certainly can't apply operations between tables
that can perhaps refine, extrapolate or summarize the data that both tables
hold. (Can't combine to pull **info** out of **data**).

**_Now going back as to how SQL Tables are different_**

Say, I have the data about employees, and those employees' addresses. What if I ask
the question _How can I get Diego's address_? Well you might have in both tables
a common field such as `ID` or `Employee ID` and by matching that specific piece
of data between the two tables, you can build your way into knowing _Diego's address_.

And so, the course will focus on how to wrap our head around the concept of SQL
tables, and how we can use them to store data, and refine it, and query it, and
overall make it work for us so that really complex pieces of info are structured
in such a way we can understand them (there's logic to them) but also are under
a framework that is extremely powerful when getting information out of data.

=== What is SQL?

Structured Query Language = It's a **programming language** used to communicate with
data **stored** in a **relational database management system**. SQL syntax is similar to
English language, which makes it relatively easy to READ/WRITE/UPDATE/TRANSLATE
data.

_S Q L_ or _Sequel_. Pronounce it either way.

Why is it called like that? It actually was created with the name Structured English
Query Language. And it was named **sequel**. But that was trademarked, and so,
to avoid lawsuits, it was changed to `SQL`.

[IMPORTANT]
====
SQL IS NOT A DATABASE. It is a command line language that many RDBMS use to access
data from tables.
====

Just like in Linux, there are 10K distros, and there are 10K DEs, under the hood,
they are still using `bash` they still share many tooling from what `GNU/Linux`
is.

Similarly, **SQL** has the same concept, this language has been designed specifically
to be used to pull, **query** data from a database. And we have tons of relational
databases out in the market (PostgreSQL, SQLite, MySQL, Microsoft SQL, etc).
_The correct way to define them with a general term is Relation Database Management
Systems_

An example of a statement is: `SELECT * FROM tablename;`. Check this statement out,
it almost reads as english, the idea is to be perceived that way. Later you will
see how there are other words that are _powerful_ operators like `WHERE`, `AND`,
`OR`, `GROUP BY` amongst others.

=== Top 5 RDBMS using SQL

- Oracle DB = Old, one of the biggest database vendor in IT.
- Microsoft SQL Server = Old, amongst the first ones actually, it's consired as
really stable, secure and reliable. It competes against Oracle and IBM DB2.
- MySQL = Swedes, community developed, open-source. SUN Microsystems bought it.
Then Oracle got SUN Microsystems. Even though this is open-source, there are
commercial licenses from Oracle. _Easy to use, inexpensive, reliable, large
community_.
- PostgreSQL = Community developed, open-source, (Successor of Ingres).
- MariaDB = Community developed, open-source, this actually is a fork of MySQL.
Because someone was concerned about the Oracle acquisition. Hence it branched off
and became its own thing. This actually was done by the original people that made
MySQL.

=== Introduction to Microsoft SQL Server

MSSQL = Microsoft Server SQL is a suite of database software published by Microsoft
and used extensively in enterprise world.

- Features:
	- A relational database engine which stores data in tables, columns and rows.
	- Integration Service (SSIS), which is a data movement tool for importing,
	exporting and transforming data.
	- Reporting Service (SSRS), which is used to create reports and serve reports
	to end users.
	- Analysis Sevice (SSAS), which is a multidimensional database used to query
	data from the main database engine.

In this course we will work with MSSQL 16, 19. And work on a Windows Server 2016, 2019.
"machine". Depending on which SQL Server version you decide to use, then you will have to
match the right OS.

Oh yeah we will use the full **Enterprise Version** for both RDBMS and OS. They
have a trial of 180 days (about 6 months). _We should finish the course by then_.

[NOTE]
====
85% of corporate companies around the world use MSSQL 16 or 19, or older, hence this
course will focus around them. (_That way you can have the biggest chances of saying
you know how to work with a version that a potential employer might have a need for_)
====

**SQL Server Management Studio** is an additional software we have to download to
be able to query data, run reports, manage tables and rows, perform backups and
analyze performance charts.

_Note:_ Even though they have a year in their name, they are not actually released
on those years.

The most recent versions are:

- SQL Server 2014
- SQL Server 2016 **
- SQL Server 2017
- SQL Server 2019 **

.Pros and Cons
[cols="2,2", options="header"]
|===
| Various supported versions   |   Expensive Enterprise edition
| Online product documentation |   Difficult licensing process that's always changing
| Microsoft premiere support   |
| On-premises and cloud database support |
| Plenty of tools and applications
| Support for use on Linux
|===

[IMPORTANT]
====
The main idea of the course is not to sell you on Microsoft SQL. But to make you
fluent in SQL. The language that is _more or less_ the universal spoken language by
all the major RDBMS out there. So if you know how to use it to query, filter, manipulate
data, you can easily jump between all the different databases.
====

=== Different editions of Microsoft SQL

- Enterprise Edition = This delivers **high-end data center** capabilities with
**blazing fast performance**, unlimited virtualization, end-to-end business intelligence.
So basically high service levels for mission critical workloads and end-user
access to data insights.
- Standard Edition = **basic data management** and business intelligence database
for departments and **small organizations** to run their applications and support
common development tools for on premise and cloud, it enables effective database
management with minimal IT resources.
- Web = **Low-total-cost-of-ownership** for **web hosting companies**. E-commerce
sites use this a lot.
- Developer = This is basically **Enterprise** but the idea is for testing purposes,
and also learning. You can't use this for production though, if Microsoft gets
word of some company using this for production. You can get easily get sued.
- Express = **entry, free level database** The idea is something small to start
with, Microsoft's idea with this tier is for people to start small, and if things
work out really well, they seamlessly upgrade this edition to the "better ones"
seamlessly.

The idea behind aiming for the **Enterprise Edition** choice of usage, is that
you will get exposed to the full capabilities of the suite that way. Since it has
all the features, and it focuses on the most complex problems, since this is the
_corporate world_ with different needs than small businesses (which are still valid
things to be respected, but the nature of the problems that each organization tries
to solve vary in scope and just overall amount of users).

=== Quiz 1: Introduction

- What is a database?
R: A database is an organized and systematic collection of data generally stored
and accessed electronically from a computer system

- What is the difference between spreadsheet and database?
R: A lot, but databases storage costs are less, and give a way better framework to
work with data so that you can really manipulate and do what you will with it.

- What is a difference between database and relational database?
R: DBMS store data as file whereas RDBMS stores data in a tabular form, AND In
database the data elements need to access individually and in RDBMS multiple data
elements can be accessed at the same time.

- In RDBMS an identical field in one table can be used to pull data from another table
R: True

- SQL stands for:
R: Structured Query Language

- Which year SEQUEL was first invented by IBM?
R: 1970

- Which of the following is NOT a relational database
R: MongoDB

- Oracle DB is an open source RDMBS
R: False

- What is the difference between SQL and MySQL?
R: SQL is a standard language for retrieving and manipulating structured databases.
On the contrary, MySQL is a relational database management system. AND SQL is used
to query and manage RDBMS.

- MariaDB is named after
R: Monty's younger daughter, Maria.

- Which of the following is NOT MS SQL version?
R: 2015

- Which of the following is Development edition of MS SQL?
R: Basically, ALL FEATUREs, but its intended purpose should be testing environments.

== Download, Install and Configure Windows

=== Welcome to Download, Install and Configure Windows

=== MS SQL Server Pre-Requisite

- Admin rights on a computer
- Hard Disk = 6 GB minimum (20 GB for the course)
- All editions of Microsoft SQL (except Express) require 1 GB of memory. Express
- 512MB, and 4 GB for Windows. We will have a Windows machine, with Microsoft
SQL installed.
- Only x64 processors are needed. 1.4 GHz, Recommended 2.0 GHz.
- OS = Windows Server 2016, or the newer 2019. Whichever is available.

=== Lab Setup

. With limited resources = You could just have your computer with OS and install
the Microsoft SQL RDBMS. If your computer is really slow, you can still use the
Express version, but many of the features of the course that will be covered won't
be accessible to you.
. The recommended approach is to **Virtualize**. We will use a virtual machine,
so that we will have a virtual machine installed with Windows Server. And we will
install Microsoft SQL Server Enterprise Edition there.

=== What is virtualization?

In the real world, in the world of infrastructures (servers), the bread and butter
is actually **virtualization**. But before, you usually just installed a OS, an app
on top of it and you were done.

This has a problem though:

- A physical server has some specs (e.g., 16 GB RAM, 4 CPU)
	- You install a OS (takes 4 GB of RAM)
		- You install services (like MSSQL)
			- And then this only takes half of all the capability of the server.
			It became a waste of resources. **Hence** virtualization came to try
			and optimize, to use that computing power and still get more value of
			it.

The infrastructure then changes:

- A physical server
	- You install a virtualization layer
		- You install tons of different OS and each one has applications running
		on it (how many will depend on the specs of the server and the requirements
		of the different virtual machines you will install)

What's the advantage? You can actually **use all the resources**. The beauty of
virtualization is also the fact that you can _three or four SOs at the same time_,
it's really important and beneficial for the world of computers (servers and infrastructures).

The main point is: **It uses all the resources of the server/machine**

=== Download and install VMWare Workstation Player

This is a virtualization layer that sits on top of your OS. (I will not, because
I'm on linux and QEMU kicks VMware's Butt)

We need the right resources:

- At least more than 4 GB of RAM (On Windows 10).
- HDD should at least have 50 GB.

[NOTE]
====
Did VMWare sell its soul or something? There's a whole website you have to go to
and register just to get an installer. Jesus.
====

=== Optional - Download and Install Oracle Virtualbox

Another virtualization software such as VMWare Player, made by **Oracle**.

=== Create a Virtual Machine

Remember:

- Computer
	- Virtualization Software (VMWare Player)
		- Virtual Machine (Container that has resources added to it). 20 - 50 GB
		of space, 4 GB RAM, 1 - 2 CPU

In VMWare Workstation you need to set the connection to Bridge, but on QEMU, things
are differently, NAT is actually what allows to connect the HOST to the VM and viceversa
on top of allowing the VM to get access to the internet.

How, we will install:
- `Windows Server 2019 Datacenter Evaluation (Desktop Experience)`
- Custom
- Allocate all the HDD
- Next
- Set a Password (Has to be somewhat complex otherwise it won't let you go past this
screen)

And yeah to unlock it is with `Ctrl + Alt + Supr`.

- Accept the default network with `Ok`
- You can disable the Server Managment to pop-up always on startup.
- On VMWare you will have to install the VMWare tools (for better experience)

- **Change the name of the machine to `LABMSSQL`**.
- Change the timezone to your timezone
- You will get a splash screen reminding you about for how long you have the
evaluation copy (and it's 180 days - 6 months). If you don't finish the course
in that amount of time and come back, you will have to install Windows Server
AGAIN.

=== Fix EFI Network Time out error for VM Player

Basically, some people get this error when trying to run the same workflow that
is shown in the Desktop.

- Head down to the physical path at which the VM is being saved.
- In that folder look for the Virtual Machine Configuration File
- Open the file and change the `firmware` key to `bios`
- The next boot should now work

=== Download and Install Windows Server (2019)

Remember:

- OS
	- Virtualization Layer (VMWare, VirtualBox, QEMU)

This is redundant (for experienced people I'd assume). It's basically the same
series of steps that we took for 2016 though. Only difference is looking up specifically
for this version instead of 16 in Google and the Microsoft Website.

=== Virtual Machine Management

How can we manage our Virtual Machine that's on the Virtualization Software?

This is obviously catered towards `VMWare Station`, but I guess the ideas that are
brought forward are things that I can totally extrapolate for any other virtualization
manager.

- You should be able to dynamically change the memory for a machine (RAM)
- You should be able to dynamically also change the disk space (expand HDD capacity)
- You should be able to add other extra interfaces (network adapter, disk reader, printer)

The idea is that VMs are way more dynamic, we can change so many things about them
on the fly, without going through the whole hassle of what an actual physical machine
would require. Hence this technology and the paradigm with which we work is so
powerful and useful.

**Note:** Most of the time you should keep your host and VM's time `synced`.

=== Quiz 2: Download, Install and Configure Windows

1. What is a hypervisor?
R: Software managing virtual machines

2. Which of the following is an example of a hypervisor?
R: Microsoft Hyper-V

3. What is a virtual machine (VM)?
R: A software emulation of a physical computer

4. Which feature allows you to capture the state of a virtual machine at a particular point in time?
R: Snapshotting

5. How does virtualization contribute to high availability in SQL Server environments?
R: By allowing quick movement of virtual machines between servers

6. What does a hypervisor manage in a virtualized environment?
R: Virtual machines

7. Which of the following is NOT a benefit of virtualization in Microsoft SQL Server environments?
R: Limited scalability

8. What does dynamic resource scaling in virtualization allow you to do?
R: Adjust allocated resources based on workload demands

== Download, Install and configure MS SQL Server

=== Welcome to Download, Install and Configure Microsoft SQL

By now we should have VM with Windows Server running. And our choice of RDBMS is
Microsoft SQL Server, so we will install it on top of this VM.

We will install

- Microsoft SQL Server
- Microsoft SQL Server Management Studio
- Configure MSSQL
- Use AdventureWorks Database

=== Which Version of SQL Server to Install?

There are many versions that have been released already:

- SQL Server 2022
- SQL Server 2019
- SQL Server 2017
- SQL Server 2016 - Mainstream support ended in July 13, 2021, but extended support
is until July 14, 2026.
- SQL Server 2014

**Mainstream Support:** There can be _paid support_ through a program and extra charges,
but this type of support basically is active, and current features can be improved and
extended, so you get more interesting things to work with. (On top of security updates)

**Extended Support:** We won't get new features or extensions to current ones, but
you will still see security updates so you have this extended time to still be safe.

So which version should you choose? It's not an easy answer.

[IMPORTANT]
====
It really depends on your business needs. Most of the time the oldest versions are
being used (because we tend to not migrate or stay up to date for very different
reasons). So it's not as black-and-white as to say _Always go for the latest version_.
Pick from the latest version one version lower, since that's where probably most
of the demand is. Besides it definitely has more time to become more stable than
the brand new version.
====

The course was made during versions 2016 and 2019. But later 2022 was dropped.

- 2019 is also widely being used now.

But just pick one version, and stick to it. That's the best advice.

=== Prerequisites for MSSQL 2016 Installation

These are good requirements for Lab and for Production environments. But of course,
for production we should have at least 6 GB of RAM (recommended), on top of (_of course_)
Microsoft's own format (NTFS, ReFS).

- Express Editions: 512 MB RAM, All other editions: 1 GB RAM
	- But recommended: 1 GB RAM, 4 GB RAM respectively
- Intel or AMD processors are okay
- And don't worry about network, that comes built-in and it's already working

**NOTE:** You need .NET Framework 4.6 for the Database Engine, Master Data Service,
or Replication.

_Considerations:_

- MSSQL 16 only works on **x64** machines
- MSSQL 16 can be installed on Windows Server 2012, 2016, 2019
- You cannot run SQL Server services on a domain controller under a local service
account
	- In English, this means that (I guess by default?), SQL Server won't run on
	the same server that has all credential and login info. (This is the Domain
	Controller, the most **sensitive and secure server**, it's literally the keys
	to the kingdom)
	- And a local service account, is the most generic type of account that can
	be registered (and used) under a Domain Controller. SQL Server cannot run
	with such an account, it needs an account specifically created and catered
	towards using the SQL Server service.
- Separate disks for Data Files, Log Files, and tempdb (This is Microsoft's
recommendations)

=== Prerequisites for MSSQL 2022 Installation

The newest MSSQL version.

So, in short. **_It requires the same things that all previous versions required_**.

_Good_.

[NOTE]
====
Okay, **ONE THING** that is different, and of course it turns like this. Is that
you can only run this in Windows 10 TH1 1507 or greater. And if this is running in
Windows Server it has to be **2016 or greater**
====

=== Download and install Chrome Browser

Okay, this isn't really one lesson on how to click some links and install. It actually
teaches you how to stop Internet Explorer of being annoying for a second.

_I'm not saying this is good, for Prod environments, definitely, being annoying is
way better than being completely laxed_.

But to stop pop-ups every 2 seconds saying that a site is not trusted:

. Open Internet Explorer
. Search for the **cog**
. Select `Internet options`
. Go to `Security`
. Select the `Trusted sites` icon
. And lower the lever of "Security level for this zone" to `Medium Low`

And when you still get the pop-ups, just check anything that says "Don't show again"
or "Continue", basically, ignore all the warnings (because we are power users and
we know what we are doing).

Even now though, you can still try and look for Chrome or Mozilla, try to download
and it won't work either. You have to go to the same screen we were before but:

. `Internet Options`
. Security
. Custom level
. Search for "Downloads"
. Mark "Enabled" instead of "Disabled"

After this, you can google whatever browser you want, and download it.

Because we are cool, we chose _Firefox_. But in a perfect world, we would go
for _Librewolf_.

It is with this new browser we should search and download "AdventureWorks Sample
Database" this is on the Microsoft Docs, it's probably a pretty widely used database
sample hence it is in Microsoft Docs even.

Download the OTLP => 2019 version. (bak file). Depending on what OS you decided to
install and what MSSQL Server you will install then you should match the specific
backup. (For the uninitiated, a `.bak` file is a typical extension for a database
backup in the MSSQL world)

[NOTE]
====
When running specifically Windows Server on QEMU, resolution might seem a bit off,
luckily by playing with the resolution settings at the VM level we can get a better
view that might suit your needs. Play around with it.
====

=== Download MSSQL and SQL Server Management Studio

Search in google "Download Microsoft SQL Server 2019". Get to the first official,
link, try to download the `.exe`, and you will have to fill in a form (literally
walled by data harvesting). After filling it in with totally real information,
you can then actually download the `.exe`.

- Open the .exe
- Select `Media File`
	- This will download the ISO for the server (and all the files that are necessary).
	And it will take some time, probably around 10 minutes or so. So just wait.
- Close
- Search for SSMS 19
- Download the installer

[NOTE]
====
If you are turning off your VM. Do clean shut downs, do not suspend or use the
Hypvervisor's controls.
====

=== Install and Configure MSSQL 2016

So a production environment might look something like this:

- Most Powerful Machines had MSSQL Server
- 96+ CPU
- 256+ GB RAM
- Solid State Drives
- Terabytes of Storage Attached
- Data files, Log Files, tempdb and backups or separate disk drives

Now, we do not have such a thing. We have a fairly small machine, with small
capacity. Still we will try to emulate a bit of how production might work by
working with different folders and labeling them as you would for different data
files, log files, etc.

**FIRST**

Let's create some folders in the C: drive.

- SQL_LOG_FILES
- SQL_DATA_FILES
- SQL_TEMPDB
- SQL_BACKUPS

This is but a naming convention the instrudctor has been following for years.
Bear in mind this is entirely theoretical, yet, you can extrapolate that into
other environments, teams, conventions. But having a sense of **ORDER**, I think
is the biggest thing you should keep in mind. There are many use cases, many requirements,
many things that actually dictate how you will structure a database, how you will
setup many devices, servers, services. But in the end, you need to have a logic
to your own madness, if you don't. Things are not going to be pretty, if things
have _too much of a lean structure_, then productivity tanks, (and many other
factors play into this, it's not just "having order or nomenclature"). But I know
from first-hand experience that followin your basics, having organization, can
be useful (_when done properly, and when not over-preparing nor over-engineering
getting in your way_).

And now we will open up the setup for the copy of MSSQL 201x we downloaded.

This opens up a wizard that's kinda craaazy, in the sense that there are tons of
options. Again, it's a toolbox, and (only someone who's insane) would know every
single thing. It's something you have to understand just about enough to navigate,
and when the moment comes, you can dive into a topic or tool and make it work
for your need. That's more realistic, pragmatic, and actually professional than
pretending that sole memorization of things without a sense of applying that
"knowledge" practically is somehow "better" or "good".

- You can also read in the MS Docs, tons of links in the "Planning" page, but a
really useful tool there is the `System Configuration Checker`. By clicking this
it will run tons of checks to tell us if the current machine can have SQL Server
installed without issues. There are tons of things to check, yet this tool automates
the checks. It's a **good practice** to always start this tool. If something is
wrong we have to go and **FIX IT**.
- Once all of that is done we simply click on the second tab "Installation" -
New SQL Server stand-alone installation or add features on an existing installation
	- You can go through the installation wizard pretty easily, yet there are some
	interesting points to highlight: You are asked for a license, in prod you would
	need to buy one and input it here. For a course, the evaluation copy should
	be more than enough.
		- This tool also runs the checks we ran before
		- We should check the option to "always check for updates"
- After continuing another set of `checks` will run. These checks are actually
focused around you solving them in order to continue, the one thing that usually pops
up here is "Firewall is on". We (for the purpose of this course) turn it off,
once that's off we have to re-run the checks, the firewall should not be an issue.
- Next screen asks for what features would you want to install. We will only be
focusing on three:
	- Database Engine Services
	- SQL Server Replication
	- Client Tools Connectivity
A cool fact about the wizard and each option you select is that there's tons of
information, like it tells you what are the pre-requisites for services, you should
install things that might be missing. It also will tell you how much everything will
take in store. These three options we selected are about 1 GB in storage.
- We will come across another screen that talks about "Default Instance" and
"Named Instance". Basically, this is a specific configuration that for specific
use case scenarios might be useful.
	- E.g., You want to have multiple sets of SQL Server installed on a machine,
	because you want a specific set to be encrypted whilst another not (this is
	but one example). And so, when you install more than one SQL Server, you will
	need to point to the `Default Instance` and the other instances will become
	named instances. There can only be **one default**. We will only work with one
	instance so we will keep it as "Default Instance". Another thing is that
	you can choose a name for your instance. However a good practice is to keep
	it as `MSSQLSERVER`.
- The next screen introduces the concept of "Service Account", and how MSSQL
adheres to best practices from the get-go. So there are these options:
	- SQL Server Agent
	- SQL Server Database Engine
	- SQL Server Browser
All these services have different roles to play when it comes to allowing MSSQL
Server to work as intended. **And**, we actually will create per-service a service
account, this is the "user" that communicates the Windows (the OS) with the Service
itself. _By default the Local System Account is used_. This is a highly privileged
service account that has access to everything. (He's admin basically). In prod
you would have to install the `Domain Controller`, and we then create Domain Accounts,
and each one with a specific access to the specific service.
	- Set all the services to be `Automatic` so that when we boot-up they also
	boot-up
	- And in Collation > Keep the default. But in short, this setting tells SQL
	Server how to share, compare and sort data based on characters.
- After that, we will setup the Configuration itself.
	- SQL Server has **two types of authentication**
		- Windows Authentication = Default one, and referred as "Integrated Security".
		This security mode is _of course_, integrated and tightly coupled to Windows.
		So specific users and groups are trusted to log into SQL Server. A Windows
		user does not need additional credentials to authenticate with MSSQL Server.
		So you don't actually need to input a password and stuffs, since if you
		have logged in, your user is used to manage SQL Server.
		- Mixed Mode = SQL Authentication plus Windows Authentication. So SQL
		Authentication is a mode that basically saves credentials **within**
		the MSSQL Server. _In Windows, we save them under the **Domain Controller**_.
			- We will actually use this. In here you can set a password, which will
			be the password for the `sa` the Server Administrator. This _of course_
			has to be **really strong**. And there's a handy button there to also
			add the current user as the administrator, that way you can login
			and have to add credentials (which is more secure).
	- After we save the credentials we can head into `Data Directories`. In here
	it allows us to map folders for different purposes to System Directories. We
	previously created 4 folders in the `C:` drive, each one will be mapped to these
	locations that MSSQL Server by default configures
		- Data root directory > C:\SQL_DATA_FILES
		- User database directory > C:\SQL_DATA_FILES
		- User database log directory > C:\SQL_LOG_FILES
		- Backup directory > C:\SQL_BACKUPS
	- TempDB = We will later explain what this is, but for now, just be sure to
	remove the default path that was there and add a new map to `C:\SQL_TEMPDB`
		- Also set the `Initial Size` (on the two locations that are there) to
		**128 MB**. This part of SQL Server is really important, it helps with
		performance basically, so you should try to make it _as big as possible_.
		It will also depend on the number of files. Depending on the cores the
		PC has, then the default number of files filled in varies `1:1`. Before
		you used to have to create all these files manually, but since that version
		the installation itself picks-up the number of CPUs and generates the respective
		number of files.
	- FILESTREAM = Nowadays, data is not just "date of birth", "name". Stuff like
	that. Data now has pictures, songs, movies attached to it. Other types of
	file formats. And so, to store them, we don't use the DB, we actually keep
	them in disk, and then we create a reference on the Database. Why? SQL Server
	doesn't allow you to create a BLOB record that's bigger than 2 GB. And that,
	of course, nowadays is _child's play_.
		- And so, for these use cases/file types. We enable the FILESTREAM setting.

Once that's done, we get a summary of all the config we established, and it will
also save an `.ini` file in disk (this is really well made dude). Once we hit
`Install` (after 10 - 15 minutes), many steps will take place.

==== SQL Server Best Practices

We will go in depth later, but it's best to understand that SQL Server has tons
of config, and depending on the environment we might have to setup things with
performance and other considerations in mind (nature of workflow, OLTP database,
BI SQL Server). Once we install the SQL Server, we tune the server after based on
such requirements. "Fascinating stuff".

After the installation is done, we actually get logs, we see everything that the
installer did, and it's AMAZING. _If there was an error in the installation_, the
logs should be a good source of truth to debug and fix.

After that also, we should search in the Search Bar of Windows "SQL Server". In there
we should be able to the `SQL Server 2008 and SQL Server 2016". 2008 should be empty,
but 2016 will have an option called **Configuration**. In there under `SQL Server
Services`, you should be able to see `SQL Server` and that `running`.

To stop, restart, the best is to come to this place and right click the services to restart.
If you manually shutdown the machine or something, data might be corrupted, and bad
things could happen. Using this interface and options will **ASSURE** that you
do a graceful shutdown of the service.

[IMPORTANT]
====
Oh wow, so now I get why the course took the time to make different videos for each
installation. In here there are less options that in `2019` for example. I was trying
to just install 2019, but now I see the difference.
====

It's nice, this is like, really nice. But, come on, a bit redundant, still it's good
to know this, and learn from my "mistake". Can keep in my head "If I want details,
I can come back to these videos for further "examination"

Okay, so since I won't lab all versions, I will take notes of the other versions that
I will "not" be installing. 

=== Install and Configure MSSQL 2019

[NOTE]
====
Props to the course. So they themselves understand that this could be dead time,
dead weight, so they went out their way to mix it up, it's not just the previous
tutorial plus some extra steps. They are adding also extra content. Wow.
====

_Short presentation:_

Before any installations, before anything that has to do with the tools. We need
to understand that a fundamental level, Databases are the key sources of data
for our business, and this in turn makes them extremely important. These are
the backbone of a business.

This is where:

- Transactions get saved
- Inventory gets saved
- Sales are getting served

Everything that is related to the business relies on the business data, regardless
of Front Office work, or back office work. Internal systems such as HR, Payroll.
They all share this same idea at their code. Data is **King**.

Because of this degree of importance, we also have to understand that the most
powerful machines in the company should be dedicated to the databases (servers).

That extra background, adds into the overview of how a production environment might
look like:

- Most Powerful Machines had MSSQL Server
- 96+ CPU
- 256+ GB RAM
- Solid State Drives
- Terabytes of Storage Attached
- Data files, Log Files, tempdb and backups or separate disk drives
- Most Powerful Machines had MSSQL Server
- 96+ CPU
- 256+ GB RAM
- Solid State Drives
- Terabytes of Storage Attached
- Data files, Log Files, tempdb and backups or separate disk drives

Again, because data and databases are so important, we also should really look
after the idea of preparing, configuring these machines and systems in such a
way that they are reliable, resilient. And recover from errors or disasters.

And now we will follow analog instructions to what we had in the `2016` section.
Download Chrome or a modern browser that doesn't have as many restrictions as the
per-default Internet Explorer (which is also a really old browser). It takes too
much time to work with Internet Explorer, doesn't really help, hence we install
another browser as the first step.

- Download SQL Server 2019. (From the official MS site)
	- Microsoft offers cloud integrations that you could pick, but in order to
	install MSSQL server we should pick the "On Premises download" option.
- The installation wizard is not just some run-of-the-mill installation program.
It has tons of tools and options that help maintain, research further and overall
support in the installation of MSSQL.
- 1443 is the default SQL Server connection. The correct _production ready_ approach
is to manually allow this port. **NOT DISABLE THE FIREWALL**. But for the sake of
the course, we will just do that. But for **security purposes** it's best to
always follow the idea of: `least privilege is best`. Also a _small note_ here,
the firewall being turned on is treated as a **warning**.
- Most important things to install
	- Database Engine Server
	- SQL Server Replication
	- Client Tools Connectivity
+
Yet, depending on your requirements, there are tons of other features and tools
we can choose to pick.
- As you continue with each step, the wizard is always keeping track of the machine
and configurations, if anything might be a problem, it **will let you know**.
- Expanding on the idea of the named instances and default instances, if you were
to configure a named instance you would have to connect to it by combining two
things: `MSSQLSERVER-test`, the anatomy is: `<InstanceId>-<Name>`.
	- We will cover how this feature is used further when we go around **replication**.
- Another reminder of the service accounts, and how a good practice is to separate
per each feature a different service account, and by pairing this with a domain
server we should provide both a user and password for them. For the purposes of
our lab though, we will use the `NT Account` which is local, and all services should
be configured to have an **Automatic** startup.
- When setting up authentication, something to keep in mind is that if you configure
your instance as `Windows Authentication Mode`, you effectively lock yourself out
of creating SQL Server users and connecting to the instance.
	- Hence, mixed mode is the best of both worlds. And in this mode you need to
	create from the get-go a SA or admin account. (Also don't forget to add the
	current user as admin)
- In `production` we will have different drives for different concerns of **data**.
	- MDF and LDF files are the ones to store data and logs respectively, in an
	ideal world, **these types of files should be on different drives**.
	- In order to mimick the separate drives, we will be using different folders
	for the different data locations (separated by concern/type)
- TempDB is really important, but its details will be explained later.
	- It's a good practice to always keep more than 1 file for TempDB, and the initial
	file size and different parameters will highly depend on the business requirements,
	(this is different to the information told at the previous section). In production
	environments files will be around 3 - 4 GB in size, and other parameters as
	well will be really buffy, we will be working with MB numbers:
		- Initial size (MB): 64
		- Autogrowth (MB): 64
		- TempDB log file
			- Initial size (MB): 8
- MaxDOP: When we run SQL Server on a computer that has **more than one core**,
it detects the best degree of parallelism. Number of processors employed to run a
single statement, for each parallel plan execution.
	- When we execute a query, we have the option of running it through multiple
	cores, so that said query also runs faster. Multiple CPUs work together in this
	instance to process the query and make it way more performant. This all depends
	on how many Logical CPUs you have on the server. (_Production numbers go from 32
	CPUs to way higher)
- Memory: This setting showed up on 2019. This is a configuration that entirely
allows you to set how much memory you want for SQL Server to allocate from the
server.
	- **IMPORTANT:** SQL Server uses a lot of memory. _Why?_ Because this DBMS
	_by design_ tries to pass data from disk onto memory, in order to access the
	data quickly. _Quick Select, Quick Update_. So the more memory you allocate
	it the better, however resources and finite, and we have to balance the memory
	used by the OS and SQL Server. A good rule is 80 - 20. 80% of memory to
	SQL Server and 20% to Windows should be a good ratio.
		- And another **good practice** is to state the server with SQL Server,
		should **only be running SQL Server**, don't try to host a web server
		or other stuff like that.
+
We will select: `Recommended` and allocate Max Server Memory (MB) to `512`. And
also be sure to tick the check mark.
- FILESTREAM = The options here basically are extra configurations that you can
add to write things to disk, allowing users to access FILESTREAM and things alike.
We will check everything.
- Again, a summary is displayed by the end with all the things you set for the
server, alongside the path of all the config in **text form**.
	- Hit `Install` and just wait, dude. Depending on the computing power of the
	machine this can take up to 30 minutes.
	- By the end, you will see a summary of all features that were installed
	(and whether their installation was successful or not), alongside a path
	to a **log file**. In case there was an issue, you should go into this log
	file and read the contents to understand better how to fix the problem.

To check if you truly have the service installed, you should open up the Start
Window and search for `Microsoft SQL`, you should get a folder with tons of
executables, but the most important one being **Configuration Manager**, this is
a program that DBAs use a lot actually, to restart, start and just manage the
service's lifecylce. And _as already mentioned before_, this is the recommended
approach when trying to interfere with the server's status. This tool makes sure
SQL Server shuts down gracefully and avoids issues by pulling the plug and corrupting
things.

By double-clicking on the respective service, you will get a Window to manage
differen aspects of it. You can choose to switch the path for logs, or features
to be turned on/off. There are tons of things that you can do here, and we will
definitely go into detail in further sections. (You can even find the path to the
log file here)

You can even go into: `SQL Native Client 11.0 Configuration > TCP/IP > Port` to
change the default port in which the service is running. (_If you do end up changing
it, you will have to restart though_)

=== Install and Configure MSSQL 2022

[IMPORTANT]
====
When we talk about SQL Server in Production environments, we refer to the most
powerful machines. They are serving trafick/data from customers and transactions
of applications. SQL Server runs on the most powerful machines in our infrastructure.
====

Of course, for our lab purposes, we won't replicate the same specs as production
machines, our VM will have the installation and usage of MSSQL Server as the
reason it has been setup, so the machine is low on specs, but just good enough for
**our purposes**.

[NOTE]
====
So, on 2022 there's a new option to choose `Developer` edition. This is the
"perpetual" version that we can use "forever". Well as long as not for production
environments. Evaluation is `90 days`.
====

It also comes with Terms and Conditions > **Accept Them**.

[IMPORTANT]
====
In production environments **WE DO NOT UPDATE THINGS WILLY-NILLY** so automatic
updates should never be set by default. The right procedure is to first update
in lower environments, test, and if everything looks fine, then production can
be upgraded knowing that we won't break it.
====

- Checks on top of checks are across the whole installation wizard. (And this is
a great idea you should be inspired by, or at the very least appreciate).
	- That's how the Firewall check pops-up.
		- **IMPORTANT:** But now this video tells a different story when it comes
		to the Firewall issue. It's more pragmatic, in a way. _It all depends on
		the business requirements_. Based on your particular use case, team,
		setup, you will probably configure the firewall differently, but a given
		is that in **production**, just having _no firewall_ is **BAD**.

(Oh wow, there's Machine Learning as a feature for MSSQL Server, _wow_).

- By default the service is installed in `C:\Program Files\Microsoft SQL Server`

Do you want to give your SQL Server the same name as the instance? Actually the
name of the instance adheres to the **name of the machine**. So if your machine is
called "TestMachine" the instance name would have that name by default. However,
if you can change the name, you should always do that.

_We tend to keep it to the default name_. The only use case in which we make use
of named instances is when we want to install more than one SQL Server instance
on **one server**. (_The SQL Server services can work independently in this setup_).

MSSQL Server => Runs as a service on a Windows Machine. And like any service,
an account has to be associated with it. Through said account is that the service
gets to actually run. _Said account needs privileges_.

We usually work (_in production_) with Service Accounts. This is with a whole
Domain setup ready. In our lab we don't have that, and won't set it up, so we
won't use Service Accounts. We will just use **the local account**.

The idea is to separate each different "service" into a different "service account".

When the SQL Server goes down, if you stop it, or if the hardware goes down and
we have to stop the machine. When the bring the server back online, do we want
SQL Server to automatically start? Then you should set that up in the configurations.

If you want it manually, then set it up as **Manual**. By default SQL Server Agent
is set to `Manual`. You can always change that to _Automatic_ though.

- MIXED MODE

[NOTE]
====
The reason as to why a good practice is to not put all the drives for the different
types of files and data inside the same Server Drive is because if that goes down,
it becomes a `one point of failure`. It's then **always** recommended to put each
data directory in different drives, so that the chances of losing the data are
less likely.
====

_It's only for our lab purposes, that we have a small machine and with limited
resources that we will emulate this separation of concerns by creating **diferent
files**_.

- On `TempDB` be sure that the Log and TempDB files are both pointing to the
`TempDB` folder.
	- Okay, params here are completely different to the other SQL Server versions.

- No 80-20 rule here for memory :(

- I don't think the other versions asked for a restart? 2022 seems to require it.

**Don't forget:** There's a log by the end. You should use the log to debug if
there was something wrong. (And please, use these things as inspiration for the
software you yourself build, imitate, take ideas that are cool, that way you will
build cool things)

[IMPORTANT]
====
Okay, I'm an idiot and forgot the password to access the machine. Luckily there's a
good way of resetting it. You just need to plug in the installing ISO for Windows
Server. And then choose to "Repair" the computer. After that you should choose
"Troubleshoot". In there a console pops-up, input this: `net user Administrator Str0ngPassWorD@`.
This is how you set a password again. And yeah, this is the password now. **DON'T
FORGET IT AGAIN**

A bit more than that though: https://www.youtube.com/watch?v=m4BBSa8uS5E[Tutorial]

Really cool way of "hacking your way" the command is actually

`net user administrator Passw0rd` => Have it just like that. JUST EASY.`
====

=== Install 2016 SQL Server Management Studio

We already downloaded SMMS, and installed it (don't remember doing that? HEH).
Apparently it takes longer to install this than the actual MSSQL Server database.

[NOTE]
====
There are tons of other database access tools. But the first-party developed one,
and of course with more features (at least up to date) will be the one that Microsoft
themselves develops. (But I want to give a quick shoutout to `DBeaver` this is insane
once you set it up correctly. You can even get DDL scripts and diagrams all reverse
engineered from Microsoft SQL Server)
====

Once the program opens up we will be greeted with a window to connect to MSSQL Server.
Now, if you choose to use `Windows Authentication`. you will see how the password
field is actually completely grayed out. And the reason being, we are already logged
in, already authenticated, so by just switching to this option. Our user will be
used to authenticate with MSSQL Server and get access to the instance.

However if we we choose `SQL Server Authenticate` we can also use the `sa` account.
And yes, it is **TONY**.

_Note:_ Oh yeah, the installation is just (Next > Next > Next).

Once you open up SMMS and then log into the instance. You get access to all the
system databases that are there by default. You also get access to the `Security`
folder, which actually shows all the different accounts to log into the instance
(Under `Logins`). And in there at the bottom, you should be able to see `sa`.
If you open its properties, you can then access `Server Roles`, and at the bottom
you should see the account with a check on `sysadmin`. This should enable the account
to literally do WHATEVER you want with the instance.

=== Install 2018 SQL Server Management Studio

_How do we communicate with SQL Server? Microsoft has its own GUI to talk to it,
and it's Microsoft SQL Server Management Studio. It used to be part of the SQL
Server installation until the 2012 version. But since then, you have to separately
download it and install it.

When accessing the official page to download SMMS, you get access to download
different management studios, all different versions, be sure that you use the
recommended versions (not previews) and ones that match the version of Microsoft
SQL Server.

[NOTE]
====
OHHH, dude, this also installs `Azure Data Studio`. This is a lightweight and
a bit cooler GUI to connect to servers. (Not just MSSQL Server).
====

The setup of this version will require to also restart the computer.

**BIG NOTE:** Oh, so like, when you are the wizard to connect, you can choose
to connect to different services, not just a "Database" **ONE** of the services
is the _Database Engine_, and yeah that is the most used one and the one that
deal with the data portion of the whole suite. But just know, that we have tons
of other features and Engines.

- If we have multiple instances, (named instance use case) we can also choose in
the connect wizard which of the instances we want to connect to. (This is used
for replication)

(_Azure Active Directory_ will be skipped for now, but that's another type of
authentication)

_Note:_ You can actually "log in twice", with the `sa` account and with the
`windows` account. And it will show you on each connection, with which account
you have logged in.

SMMS is the place that we will use to connect to MSSQL Server, it has all types of
information and ways to leverage the database features.

**There are two types of databases:**

- System Databases = Installed automatically with the setup we did when installing
Microsoft SQL Server
	- master = main database
	- model =
	- msdb = setup jobs and ssi packages
	- tempdb = the temporal database
These are all system databases, and we should back them up just as we backup user
data databases.
- User Databases = These are databases that deal with actual user data, and we
create all of them manually

Using GUI you can create databases pretty easily, literal right-click, go through
wizards. _Typical Microsoft pattern_.

- Security Tab = In here you can see all the users (logins), you can see all the
roles that we can configure for users on a database
- Server Objects = In here we can specify linked servers, triggers, etc.
- Replication = If we want to replicate data from one server to another server
- High Availability
- Management = In here you can see tons of options, but one **really important**
option here are the SQL Server logs. If there are any sort of issues with the
database, you can go into the logs here and start troubleshooting what the heck
is up fam.
	- Maintenance Plan = These are for backup management
	- SQL Server Agent = Service which comes pre-installed, the Agent is in charge
	of scheduling the jobs. Jobs that we are going to run on different schedules.
		- Backup every 2 am (for example).
		- We can create scheduled jobs (e.g., Okay at 2 am every day, run this
		script, automations are built-into SQL Server). Nothing by default, we
		setup everything here.
		- We can setup alerts
		- Proxies

=== Install 2019 SQL Server Management Studio

Repeated info. But extra:

You can just right click on a database and then choose `Run Query`, after you
do that you can run a pretty generic command: `SELECT @@VERSION`, this command
will show you immediately what's the version of the MSSQL Server instance.

The interface tells you a lot of info:

- What databases you have
- What tables a database has
- Logins allowed to connect to a database
- Server Objects

We will be looking into all these features and details further in the course.

=== Download and Install AdventureWorks Database

This is a sample database used for learning purposes and to test different
features that Microsoft SQL Server provides.

You can Google it like: `AdventureWorks sample database`, and under a Microsoft
Docs page, you should be able to see a table with tons of download links. Depending
on your database version you should download the respective backup. (Also be sure
to download the **OLTP** version).

- Once you download that, move the `bak` to the backups folder on our machine.
- Log into the SMMS application. (It would be recommended to login with Windows
Authentication though, it has more auditing around it.)
- And now to restore the backup:
	- Right click under `Databases`
	- Restore Backup
	- Select `File`
	- Search for the backup that should be under the `BACKUPS` folder
	- Once you select it, the folders for both `DATA` and `LOGS` should be automatically
	mapped to the ones we set at the installation step
	- Click `Restore`
	- Once the restore is complete, you can look at the architecture of the database
- This test database has `Tables`, `Views`, `Stored Procedures`, `Security`

Let's run our first `SELECT`.

- New Query
- Be sure you double-clicked the database and chose `New Query`, or in any case:
	- `use AdventureWorks2019`
	- go

[NOTE]
====
On SMMS you don't have to write semi-colons as you would in MySQL, Oracle. You
can just put `go` at the next line.
====

Okay, this is _cool_, didn't know you could do that with SQL Server. You can literally
Drag and Drop a table into the `Query Window` and it will immediately write/render
that table's name (a full access to it). So you can literally write queries with
motions. That's _cool_.

So the first query we will run is

`SELECT * FROM [HumanResources].[Employee]`

And this should render a all the records from the table at the bottom. You can
play around and do selects to all other tables to get a feel for it too!

=== Download and Install AdventureWorks 2022 Database

So with each new Microsoft SQL Server, Microsoft creates a new backup that has
differences in its architecture. So depending on your version you will see
a different structure.

_NOTE:_ Okay this is... interesting. But basically, for the most advanced features,
we will leverage 2022 and its AdventureWorks sample. (Why is that? No idea, still
hopefully we still get the chance to lab all those lessons, unless I want to run
another install for 2022).

- The backups that are provided for AdventureWorks are actual backups of AdventureWorks
that are free and used for learning and labbing.
- `Device` is the option with which you can point to the path at which a backup
is saved.
- When restoring a backup for a Database that doesn't exist. Then SMMS will take
care of both creating the database itself, and then putting all the values that
are respective.

[IMPORTANT]
====
There's a button on the `Restore` screen stating `Script`. This will actually
generate SQL that takes care of the restore of a backup. You can run the command
or just press buttons with the GUI. (Two alternatives)
====

Of course, _make sure the data is there_. Run a couple of `SELECT` statements and
stuffs.

== Database Fundamentals and Design

=== Welcome to Database Fundamentals and Design

Let's recap a bit:

- Section 1: Data, Databases and Relational Data Base Management System
- Section 2: VM, Windows Server Installation and managing Lab VM
- Section 3: Download, Install and Configure MSSQL Server

_What will we cover?_

- What is Data?
- What is Database?
- How Data is Stored?
- What is Table, COLUMN and ROW?
- What is a Key?
- What is a Primary Key, Foreign Key, Unique Key, etc?
- What is a Transaction and ACID properties?
- Database normalization AND different forms of Database Normalization
- Create your first Database, Table and Populate table with data...

=== What is Data?

When we use the term _database_, we inevitably make the connection to the word
_data_.

**Data** is a _collection_ of facts, numbers, words, measurements, observations
or descriptions. However this data starts as **raw**, something that might not
have that much meaning outside of context or _pre-processing_, once we process that
data, we can turn it into **information**, which is actually data with meaning
and with the potential to give us insight into conclussions that can be drawn.

In the context of databases, _data_ is referenced as any single item that can
be stored in our _database_, this can be a single item or a set.

=== What is Database?

_Base:_ Data-base => A structure that holds data.

=> Organized collection of structured information or data, typically stored
electronically in a computer system.

We typically also have a DBMS (Database Management System) that helps us interact
and manage said database. There's also _systems_ that are interconnected to the
database that provide different functionalities around **data**. All of this group
of system and subsystems, are still referenced as a Database, or Database system.

Example:

When going to the Dentist for an appointment, and a recepcionist looks up your
name, more than likely this is on a file written somewhere, in said document there's
tons of info in regards to you. Name, Last Name, Drugs prescribred, age, gender.

All of this info can be extrapolated and put in a database, the data, the underlying
information will be the same, but the value, the important piece of abstracted
knowledge, can change its form and become electronic, and be represented in
`Tables` and `Columns`. Each column can be of a certain type, and can house
a specific piece of data that brings together a greater whole, and we can have
more than one table leveraged in order to represent all of the data that the
traditional _real-world_ process employs.

Many traditional file systems, move into Databases or tend to do so. And it doesn't
matter how many tables or columns are employed to represent the original process,
as a whole, everything is referred as a database (_system_)

_Why not just a Spreadsheet?_

- This all depends on convenience and how all data is stored.
- Who can access the data.
- How much data can be stored.

Spreadsheet is used is great for a small group of people, that also don't have
to do a whole lot of complex data manipulation.

Databases => Designed to hold a much greater amount of data, and in organized
amounts. (Terabytes of data). Databases => Allow multiple users to connect to
them, and run queries that also can become highly complex yet with an almost
human readable high level query language.

=== How Data is Stored?

_This is not referring to saving data to HDD in binary_.

This is how data is stored within the database.

. Data is stored within tables (the basic unit of storage)
. Structured Query Language (SQL). In order to query data in a structured manner,
we need to save said data in a structured way. (We normalize the data through tables)
. **Tables are not databases**. See the database as a folder, and there are files
inside of a folder, the tables are these **files**, all data is written into them.

=== What is a Table, COLUMN and ROW?

A database is made of many components, one of those is **Tables**. Without them,
we wouldn't have much actually, tables are the essential structure in which data
is stored.

- There can be more than one table
- The data is stored in these tables, tables are like _placeholders_
- A table should have a **unique name**
- The number of tables is limited by the number of objects allowed in the RDBMS.
For example Microsoft SQL Server has a limit of more than 2 billion tables (which
is kinda impossible to reach)
- MySQL has no limit

**What is a Column?**

A set of data values, all of a single type, in a table. E.g., Age, name, Department,
salary.

- Each column tells you what type of data is going to be saved in that column
	- The name should imply what type of data is going to be saved
- A column may contain all sorts of data types, text values, numbers, or even pointers
to files in the system
	- You can put dates, even currency specific types, decimals, precision
	- **IMPORTANT:** The column names should be unique
- SQL Server => You technically have 1024 columns. It's not practical to have as many
though, 30 - 40 columns are extreme cases. This is where normalization comes into
picture.
- There should be _at least one column in a table_
- Field = Column.
	- However, some developers might refer to a Field as the intersection of a
	Row and a Column.

**What is a Row?**

Collection of fields that make-up a record.

E.g., We have a row that tells data such as `EmployeeNo, Name, Age, Department,
Salary`.

- A row is also called a _record_
- A row should be unique (when all of its values are combined)
- A table can contain 0 or more rows. When there are no records, the table is
referred to as _an empty table_
- The number of rows is only limited by the storage capacity of the machine (unlike
Excel as an example)

=== What is a Key?

Tables store a lot of data in them, most of the time these records are unsorted
and unorganized. If you want to filter data, you can try and make usage of some
column's value, but if those values are repeated, you immediately will run into
an issue such as records that you don't want popping up as results.

To avoid all these issues, **Keys** are the solution, this is a way of making a
record truly unique, and reference it by that unique **value**.

- A key is a data item that exclusively identifies a record.
- E.g., You have a table of students, and there are more than one "John" students,
you can pinpoint to the one you really care about by using his `StudentId`
- Key can be a single attribute of a group of attribute (composite key)
- Keys are also used to generate relationships amongst different tables, tables
most of the time are connected to each other and that is through the keys, for
that purpose you use a _foreign key_.

[IMPORTANT]
====
A key is an important component of a table. And it's used to exclusively identify
a record whenever we are trying to query for it.
====

=== What is a Primary Key, Foreign Key, Unique Key, etc?

First, let's run a quick exercise. In a table with no concept of keys introduced
yet, what column would be a good key?

**Candidate Key:** An attribute or set of attributes that uniquely identifies a
record. (Many columns cannot be used alone to try and uniquely identify a record,
hence those wouldn't be good candidate keys)

In a table such as:

[cols="1,1,1,1,1"]
|===
|StudentId | Name | Class | DOB | Email

|1001
|John
|1st Year
|01-08-2008
|john1@gmail.com

|1002
|Roger
|2nd Year
|01-08-2013
|roger@gmail.com

|1003
|Michael
|3rd Year
|02-02-1993
|michael@gmail.com

|1004
|John
|3rd Year
|02-03-1992
|john@gmail.com
|===

Candidate keys, could be `StudentId` and `Email`. But nothing more than that,
_why?_ Because they are unique amongst records. Any other value can be clash
easily, (you can already see two johns there, and there could be people with the
same birthday, and perhaps same name and same birthday)

- Tables can have multiple candidate keys
- Amongst the set of candidate keys, **one** is chosen as a **Primary Key**.

_NOTE:_ When naming tables, DBAs tend to name tables with **Camel Case**. E.g.,
`studentInfo`. This is but a convention, and in real world, you will definitely
see all manners of conventions. But it's good to know that there is a sense
of "convention" and people tend to follow that.

- Primary is a set of one or more fields (columns) of a table that uniquely
identify a record (row) in a database table.
- A table can only have **one primary key**, and you should pick one _candidate
key_ as that.
- A primary key should be picked only if its value is rarely or never changed
- A `primary key` cannot be `NULL`
- Primary key fields contain a clustered index

**Secondary Key** => All candidate keys that were **not selected as primary keys**,
you could use them as ways of identifying a record, but again, _in practice they
are not the primary key_. Sometimes they are referred as **alternate key**.

**Uniqe Key:** Set of one or more attributes that can be used to uniquely identify
the records in a table. They share the same attributes as **primary keys**, however
these columns can have `NULL` values. A unique field containst a _non clustered
index_.

**Composite Key:** A combination of one or more attributes can make up a "primary
key". A composite key can be a candidate for a `primary key`. E.g., StudentId +
Email can become a composite key.

**Foreign Key:** A field in a table that is a **primary key** in another table.

E.g.,

.studentTable
[cols="1,1,1"]
|===
|StudentID, Name, Class

|1001, Diego, 3rd Class

|1002, Richard, 1st Class
|===

.invoiceTable
[cols="1,1,1"]
|===
|StudentID, InvoiceID, Cost

|1001, 1, 3000

|1002, 2, 3500
|===

As you can see, we have _two tables_ and on the second one, we have the `StudentID`
as a column. This is actually a reference that ties the record in the `invoiceTable`
to a student record in the `studentTable`.

- A foreing key is used to generate a relationship between two or more tables.
	- The way to trace back all the way to another table is to pick the foreign
	key column value and search it in the respective table
- A foreign key can be duplicated, and it can have `null`

[IMPORTANT]
====
The concept of keys plays a very important role when designing databases. They can be
of much help when running queries, and good design on them can make life easier
when managing a database.
====

=== What is a Relational Database and Relational Database Management System (RDBMS)?

Relational Database = Type of database, based on the _relational model of data_.

The idea is to store and provide access to data points that are related between
each other. To ensure the data is always accurate and accessible, relational
databases follow certain integrity rules.

Logical data structures => Means that we abstract things, and are separated from
the actual physical hardware. The relational data model was designed to resolve
multiple data structures that in old times were used arbitarily by engineers.
These structures were verbose, complex, and hard to maintaint. So the idea of
the relational model with its **Table** structure was a way more intuitive, simple,
and extendable approach to data storage.

_Relational databases_ => They have **Relations** as the key concept between them.
Keys are the columns that manage to connect/relate different records from different
tables.

A relational database management system (RDBMS) is a software that allows you to
manage a RDBS. Most RDBMS use the SQL Language to access the database. Meaning that
regardless of what RDBMS you are using, the language should be pretty similar,
meaning your knowledge can be transferred over pretty easily.

**Advantages:**

- Enhanced data security (authorization, access control can support encryption that
is strong). You can even configure what data is accessible to who. So for businesses
that require control of the data and its access, Relational is good
- Retain Data Consistency => Because of the ACID properties, data can be assured to
be in a consistent form
- Better Flexibility and Scalability => Updating can be done independently, that
is because modifications can only be made once, you change one place (one table)
and that value will then ripple across all other data access points
- Easy Maintenance => You can regulate, fix and backup data pretty easily. And
that is because there are automation tools that do all of this easily

=== What is a Transaction and ACID properties?

Transaction => In day to day to live, we often here "Transaction" in DBMS, this
is a single logical unit of work which accesses and possibly modifies the content
of a database. A transaction accesses the data with **write** and **read** operations.

A transaction groups SQL statements so that **ALL** of them are comitted, or **ALL**
of them are rolled back (means they are undone).

_When do you rollback transactions?_ When there is a **failure**.

ACID => Concept, or acronym to the 4 properties of a transaction in a database
system.

Atomicity
Consistency
Isolation 
Durability

If you follow these four properties data can remain consistent and accurate on
a database, the data should be become corrupt in case of some failure. And
lastly, because of ACID, we can focus on the application logic instead of
failures, recovery and sync of data. (_So the idea is to abstract more, and let
developers do more of other value adding tasks_).

==== Atomicity

This means, that a transaction needs to be an atomic unit of work. Meaning, either
**all of the operations are executed** or **NONE**. So if one part of the query
fails, then everything else will immediately fail, in case you encounter an error,
none of the changes will be commited.

==== Consistency

The database must remain in a consistent state after **any transaction**. So
a transaction needs to ensure that it adheres to **integrity constraints**. That
way the data should remain _consistent_. The data has to remain "correct". When
trying to add/modify a value to a column, depending on its constraints.

So if a column can't be negative, this constraint should be enforced, and fail
a transaction if trying to do such a thing.

==== Isolation

The transaction will be carried out, as if it was **the only transaction** in
the system. Meaning that one transaction should not be changed, influenced or
changed by another concurrent transaction (but this is bs, innit? Deadlocks, stuff that clashes
does happen, specially in _production_).

_So in a perfect world, a transaction should wait for another transaction to act
on a record before trying to manipulate any of its data_. In order to avoid
inconsistency, databases put locks on records, that way one transaction has a
sort of **reserved spot** until its done with its operations.

=== Durability

Once a transaction is comitted and applied, then its changes are kept **permanently**
in the database. Hence databases should make use of Hard Disks, that way if the
power goes out, then when coming back online, the data should still be there,
the transaction's changes should remain, be _durable_.

[IMPORTANT]
====
ACID is just a theoretical way of looking at it. It's a great start, concept and
core values to have. But in practicity you run into all sorts of scenarios and
weird things that do prove the rules that they are not so _black-and-white_. You
have to make concessions, and sometimes just accept that guidelines or rules
inevitably end up broken due to specific business requirements or scenarios,
or perhaps there are other issues at hand, and you should be prepared to deal
with those issues.
====

=== Database Normalization AND different forms of Database Normalization

_Why do we need normalization?_

To avoid redundancy on data, (having a table with records that save the same
data for different columns). We are not really optimizing the usage of storage
space, we **waste disk space**. Maintenance also becomes harder, say you want to
update the details of a customer, with a bad structure, you would end up having to
go to all sorts of places and iterate across tons of records to change each one
in order to keep data consistent. That's **BAD**, you should always aim to have
_one place_ that can get updated, and then all of that info remain consistent to
any of its relations.

==== Normalization

Process that organizes the data in the database in order to reduce data redundancy
and increase the data integrity.

- It's a systematic approach of decomposing tables to eliminate data redundancy
(repetition) and undesirable characteristics like Insertion, Update and Deletion
Anomalities
- This is a multi-step process that puts data into tabular form, and aims at
removing all manner of redundant data from relation tables.
- This works with a series of _Normal forms_.
- Although other levels of normalization are possible (1NF, 2NF, 3NF), the third
level is the highest level considered to be necessary for almost all applications,
_so if you go above that, then you are just trying to show off_.

**1NF**

- A relation is in this normal form, only if **it contains atomic values**. Means
it has a value that cannot be divided. Also, if there are no _repeating groups_.

So, in order to normalize with 1NF, look out for rows that have repeated values
in its columns, and after that, inside each value, check whether you are saving
_more than one value_ in a column, that is bad, you should probably put each value
in a separate column or perhaps making another table to keep tabs on these multiple
values that could be qualified as a specific field.

**2NF**

- If it's in 1NF, and **all partial keys dependencies are removed**. When an
attribute depends only on a _part_ of a primary key, and not the whole key, we are
**bad**. In _English_. If you have a record that does indeed reference a key,
but also info about this key is getting repated with each record, then that means
that this is a **partial key dependency**. You should separate such a case
in **2 different tables**. That way you only keep references to keys, and on
each separate table said kays can have all the attributes they want.

**3NF**

- If it's in 2NF, Non-Primary Keys columns shouldn't depend on the other non-
Primary key columns.
- No transitive functional dependency

Okay, in _English_. Basically your columns, should not depend between each other.
E.g., A `courseCode` column, depends on the value of a `majorCode`. Firstly,
**you can't make either of these fields a primary key**, cuz changing one of
its dependencies can immediately break the records way of being referenced by
other records. And even if you don't have a direct dependency between two columns,
if there's a **third** column thrown into the mix, and it has a dependency,
the _third column_ indirectly will also become dependant on the dependences of its
own _first-class dependency_. To avoid all of this **non-sense**, you should separate
the table columns into different tables, and basically just assign their own
primary keys, that can then be later referenced on the other table.

_In short:_ Always, look out for maintainability, look for problems that might
arise with the design decision you make, (normalization is done at the design
stage anyways). Normalization should help a lot with a good design. After you
identify a problem for a **ETC** EASIER TO CHANGE later situation, then split
things into smaller tables, try to reference keys always, avoid dependencies
between columns, and avoid data redundancy. **That's the essence, the key to
normalization**.

=== Create your first Database, Table and Populate table with data

Okay, let's actually get our hands dirty. Just boot up the VM. Don't forget the
Passw0rd DUDE :D.

_Note:_ It's better to use the Windows Authentication to log into the SQL Server
instance.

_Note 2:_ Also don't forget about update management, you should update, test that
in lower environments, and once everything seems in order **THEN** update prod.

We will now create our database from the group up, you can do it with a script or
with the GUI. (Which apparently it's easier, _yeah no_). Anyway, once you right
click Databases and select "Create Database", in the wizard you will pick a name
and later you will choose an **Owner**. A good practice is to either leave the owner
as `Default` or add `sa`. Don't put a single user for it (in practice this is just
not true, what if we want granular control with service accounts?)

When configuring the DATA database and the LOG database, you have several parameters
you can fine tune depending on the use case. But the initial file size for `DATA`
for example, we will set as `64 MB`. **Autogrowth**, is a setting that we can fine
tune depending if we are on PROD or DEV, you can even limit the size of the database,
this is also dont on lower environments, and mostly when the need arises that wants
for you to re-use a server for multiple other applications so other databases and
you are organizing how much will **this** database actually take in disk. In PROD
however we should keep it as `unlimited`. With this setting, the moment we reach
the maximum, any insert will start throwing an error. We will set its limit to 1
GB due to our VM having limited resources.

There's also the option to change the `Path`. However, we already set the default
path and that gets applied, (still you have the choice of changing to another
_path_).

- We also get to configure the LOG database, which will be covered in detail later,
but it's basically really useful when restoring a database after a failure or
outage.
	- We will also limit the LOG to half a GB

- If we head down to `Options` now, we will get a plethora of things to configure,
these will be covered later, but just for now let's explain extra things:
	- Recovery Mode: Full or Simple allows you to go back in time and restore the
	database at **one** point in time. E.g., A banking database, a failure is
	detected, a user registered a value that was invalid in a table. We can go
	back to a minute before that catastrophe (Only in Full Recovery Mode) which
	in turn also keeps a lot of LOGS. In our case, our database will be simple,
	so we use the **SIMPLE** option
	- We can just press "OK" and then the wizard will make sure of running all
	queries under the hood to get the database up and running with all the configurations
	we set out.
		- But an extremely powerful tool is the "SQL Script" button, if you press
		this and select the "In new Query Window" option, then you will be put in
		a SQL Query Window with all the scripts that will be run in order to create
		the database and its configurations.

There's a whole structure to the SQL Query and many parts to it. You can run all
the queries with `Ctrl + E`. Now, a basic way to use the SQL Query Window is select
everything, select nothing and run with `Ctrl + E` or just press the `Execute`
button. _The best practice is select what you want to be run and run it_.

From the different options you can make out the recovery mode, the autogrowth,
all of that. At first, if you don't see a database, then **refresh**.

We will now create a table with the GUI. `Right Click on Tables > New Table`.

The columns we will create are:

- firstName, varchar(50), nullable
- lastName, varchar(50), nullable
- dob, datetime, nullable
- ID, int, not nullable

If you choose to close the window now, it will ask you if you want to run this
creation, alongside the table name, we will put it as `personalInfo`

After refreshing, you should be able to see the table, also its columns, a note
here, the order of columns in the GUI is not really important. We can create a
script for existing tables also, SCRIPT Table as => CREATE. This will then show
you all the SQL that was run in order to create the table, it's pretty handy and
you should always look at this, in production and real life, GUI is way less practical,
and deployments and configurations done through an Infrastructure/Server team will
always rely on scripts, TEXT files is just a powerful way of transmitting files,
versioning, and overall acquiring information while reading.

Oh, right, on the wizard you don't get an option to set a primary key, HUH. So
after we create our table, we actually don't have a PK yet. Yo can make edits
to an existing table by `Right Click > Design`. You can then select the box
to the left of a column and "Set as Key". Once you try to close the window
it will ask you to run stuff.

Once you run, you can again try to generate a `CREATE` SQL script. You will see
how it should have a new statement that's all about making the ID column the
**primary key**. Now, if you wanna autogenerate an `INSERT` you can use the
`Right Click > Script Table as > Insert To`. This will generate a template you
can use for inserting data, for our purpose we will just write our info there to
insert, if the query has any syntax error the window will let us know, and stop
us from trying to run a malformed query, until we fix the syntax then we are
protected against that possible mistake.

**Notice that this is only syntax, if a value is wrong or whatever that falls on us
to double check**. Once you run the insert you should see `1 row(s) affected`.

Now if you wanna see the data of the table, `Right Click > Select Top 1000 rows`.
Or you can write `SELECT * FROM personalInfo`. Either approach works fine.

[NOTE]
====
In SQL Server we tend to separate statements with `GO` not with semicolon. Although
it's supported and you should be fine.
====

You can also edit values with `Right Click > Edit Top 200 Rows`. With this you will
get a GUI to edit values in wizard. After all your changes, if you use the **keyboard**
with `TAB` then the changes to the rows are automatically applied. But _please
don't use this approach_. It's there, but it's best to use INSERT or UPDATE statements.
The GUI is **too error prone**.

=== Quiz 3: Database Fundamentals and Design

What is the primary purpose of a database? 

- To store and organize data

Which term is used to describe a collection of organized data? 

- Database

In a relational database, what is a column often referred to as?

- Field

What is the smallest unit of data in a database table?

- Field

Which of the following is an example of a non-relational database model?

- MongoDB

In a relational database, what does a row represent?

- A record

Which SQL keyword is used to retrieve specific data from a database?

- SELECT

What does RDBMS stand for?

- Relational Database Management System

What is the purpose of the "WHERE" clause in a SQL query?

- To filter the rows based on a condition

Which term is used to describe a unique identifier for a row in a table?

- Key

== Introduction to SQL Commands

=== Welcome to Basic SQL Commands

This section will be about the basics of SQL Commands. We will learn about
a SQL statement, types of SQL statements (how they get used and examples, we will
run labwork to see how they perform as well)

=== What is a SQL Statement and types of SQL statements

[IMPORTANT]
====
Forgot where I placed this note, so I'll keep writing it as long as I need. If you
restart your PC, you need to start two services to use QEMU on Linux:

- `sudo systemctl start libvirtd`
- `sudo virsh net-start default`
====

SQL = Structured Query Language. Used to communicate with a database. It's this
language that we use to communicate with the database. _Actually_ according to
ANSI (American National Standards Institute), it is the standard language for
**relational database management systems**.

We use SQL statements to perform operations on data, such as update on a database,
retrieval from database. Also create users, grant permissions to users and all sorts
of stuff like that.

**Most common RDBMS**, like MariaDB, MySQL, Oracle, all use SQL. However, they
also have **extensions** to the base language. Queries, keywords, or functions that
are only specific to **that** RDBMS.

However these statements are the ones that we can be sure exist in _most_ if not
_all_ database relational systems. And we can do almost everything we will need
with them:

- Select
- Insert
- Create
- Update
- Delete
- Drop

These are the basic commands, **_universal_**. There might be a difference in
syntax, but the purpose and functionality should still fall within the usual
standards.

Some examples:

[source, sql]
----
USE AdventureWorks;
SELECT * FROM Sales.SalesOrderHeader
WHERE [OrderDate] BETWEEN '1/1/2021' AND '4/31/2021'

USE AdventureWorks;
UPDATE Sales.SalesOrderHeader SET OrderDate = getdate()
WHERE [OrderDate] = '1/1/2021'

USE AdventureWorks; <.>
DROP TABLE Sales.SalesOrderHeader
----
<.> We rarely do these kind of statements and in this structure, at least in
production/real-life scenarios. But, still, it's good to know a bit about the
different things that are possible within all of this.

All these statements perform different operations, but if you try and read them
like normal english, you should be able to make a pretty good educated guess
as to what the statement is doing.

We actually saw different type of SQL statements. There's a classification:

- DML (Data Manipulation Language) = SELECT, INSERT, UPDATE, CREATE
- DDL (Data Definition Language) = CREATE TABLE, CREATE DATABASE, ALTER TABLE
- DCL (Data Control Language) = GRANT ACCESS
- TCL (Transaction Control Language) = These are statements that help with ROLLBACK
and SCOPING

=== DML Statements with examples

Data Manipulation Language = As its name implies, it's a subset of the SQL universe
focused on manipulating data. Actually most of the most common statements are under
this category, SELECT, INSERT, UPDATE, DELETE, etc.

_We use these statements, to insert, delete, update, create records in a database_.

**SELECT statements**

A statement used to get records from a table, _with or without conditions_

- SELECT * FROM student - Gets all records of a student table
- SELECT * FROM student WHERE rank > 5 - Gets all records with the condition where
student's rank is greater than 5

**INSERT statements**

Used to insert a set of values into a database table. INSERT statement is used
with `VALUES` as a pair of sorts.

- E.g., `INSERT INTO Student(Rank, StudentName, Mark) VALUES (1, 'Abbas', 450)`

With this statement, a record will be inserted into the Student table, and looking
at the structure, we provide all the columns we want to insert a record into,
followed by the actual values that are mapped **respectively**.

**UPDATE statements**

Used to update an existing value on a table, _based on a condition or not_. Usually
we make updates with `some kind of condition`.

- E.g., `UPDATE student SET StudentName = 'Abbas' WHERE StudentName = 'Imran'`

[IMPORTANT]
====
In the previous statement, if we didn't specificy a condition, then as a result we
will be updating the whole table on that specific column to a specific target value.
In real world scenarios, this could be catastrophic, hence **always keep in mind**
if your update has the correct condition set for it.
====

**DELETE statements**

Used to delete an existing record in a table. This is usually based on a condition,
and it follows the same idea as UPDATE actually, _if you don't want the statement
to delete everything within a table, please **add a condition**_.

- E.g., `DELETE FROM Student WHERE StudentName = 'Abbas'`

This will delete the complete row that satisfies the condition. (If there are many
records all of them will be deleted, **keep that in mind**)

=== DDL Statements with examples

Data Definition Language, this type of language deals with database schemas and
descriptions. They basically help declare **how** data should reside in a database.

- `CREATE, ALTER, DROP, TRUNCATE`

**CREATE statements**

Used to create a new table, database, user in the database. This can be used to
create other objects like stored procedures, functions, and others...

This helps defines objects and create them in a database.

- E.g., `CREATE TABLE Student (Rank Int, StudentName varchar(50), Mark Float)`

_Personal comment:_ Okay this is pretty succint and clean, didn't know you can
group them together like that for a CREATE TABLE_

So yeah, you can declare the name of a table, and then all the columns alongside
their type.

**ALTER statements**

This is used to _change_ an existing object. These can add a column, modify, drop,
rename, rename an index, change the name of a database. Its scope is actually
**really big**. It can be used to modify most if not all of the database objects
that are out there in SQL Server.

You use this to **modify an already existing object**.

- E.g., `ALTER TABLE Student ADD (StudentAddress varchar(100))`

Pictures this use case: We have an already running system and it has a Student
table, with records that are of course precious to us. We can't just drop the
table and lose all of that for a modification that we want to make (adding a new
column). `ALTER` comes in pretty handy to yes, mutate the table's structure with
a new column, but keeping the records in it.

**DROP statements**

Used to drop objects in a SQL Server. Remove a table definition. _NOTE: Don't
forget that when you drop a table definition all of its data also is deleted (dropped)_

You can drop index, trigger, constraints, permission specifications for a table.

**NOTE:** These statements are generally regarded as kinda dangerous, so really
think twice and how you are using them to manipulate a database structure.

- E.g., `DROP TABLE Student`, `DROP DATABASE AdventureWorks`.

**TRUNCATE statements**

Removes all rows from a table, without logging the individual row deletions.

_Remember how we talked about `DELETE`?_ Well `TRUNCATE` is similar, but the
**key** difference is that DELETE statements are logged in the database. Technically,
because there's a log, we can actually roll-back that execution if something happened.

But a `TRUNCATE` won't register any logs. Once we run this statement, **there's no
way to recover DATA**. _We could also save time since the logging overhead is not
there_.

[IMPORTANT]
====
In real life world scenarios, DBAs tend to use `TRUNCATE` when the performance of
the deletion is time-sensitive/critical, to the point that we can't afford all the
logging overhead of a `DELETE`.
====

_Another difference:_ `DELETE` has the abilities to select a row we filter (condition)
so that specific rows are affected, however `TRUNCATE` is pretty simple. (You simply
delete everything at once)

- E.g., `TRUNCATE Student`

=== DCL Statements with examples

**Data Control Language**

- It defines the control over the data in the database, e.g., `GRANT, REVOKE`,
wich are used to grant access or revoke access from a user.

**GRANT Statements**

GRANT is used to grant SQL `SELECT, UPDATE, INSERT, DELETE`, and other privileges
on tables and views.

- e.g., `GRANT UPDATE ON ORDER_BACKLOG TO JONES WITH GRANT OPTION`. This statement
for example, **yes** it does allow `JONES` to do updates on `ORDER_BACKLOG`, **and**
it also lets him `GRANT` this same permission to others. (So you see that this is
extremely granular, or well, can be).

- `GRANT SELECT ON TABLE Q.STAFF TO PUBLIC`. This statement acts upon a **Group**
actually. Any person that is part of that group, can now do `SELECT`s on the
`Q.STAFF` table, but no one can gran that same permission to others.

**REVOKE Statements**

This is used to cancel previously granted or denied permissions.

- E.g., `REVOKE DELETE ON employees FROM anderson`. Here, we take away the permission
to `DELETE` from `anderson` on the `employees` table.
- E.g., `REVOKE ALL ON employees FROM anderson`. Here we take away **all possible
permissions on the `employees` table from `anderson`.

=== TCL Statements with examples

**Transaction's Control Language**

- Used to manage the transactions in the database
- Used to manage the changes made by DML statements
- It also allows all statements to be grouped together into logical transactions,
meaning that tons of statements can be executed as _one transaction_.

**COMMIT Statements**

- This command is used to permanently save any transaction in the database
- When we use COMMIT in any query, then the changes made by that query will be
permanent and visible
- e.g.,
+
````
BEGIN tran d
UPDATE emp SET empName='D' WHERE empid = 11
commit tran d
````
The way to understand this SQL statement is pretty simple, you create in memory
a transaction object with an alias (variable name), and then wrap as many DML
statements as you want, when SQL Server runs all the query, everthing that executes
inside of a transaction is only **in-memory**, it isn't yet "official", the moment
we hit a `COMMIT` that's when all those changes that were memory-only are then sent
to be written to disk.

**ROLLBACK Statements**

- Used to undo the changes made by any command, but only before any commit is done.
Meaning, the moment info is written to Disk, we **CANNOT** roll that back. That's
out of this statement's capabilities.
- If a `COMMIT` has already run, we cannot roll it back

This is a bit verbose, but a good example of how you would leverage a ROLLBACK:

[source, sql]
----
DECLARE @BookCount int <.>
BEGIN TRANSACTION AddBook <.>
INSERT INTO Books VALUES (20, 'Book15', 'Cat5', 2000)
SELECT @BookCount = COUNT(*) FROM Books WHERE name = 'Book15'
IF @BookCount > 1 <.>
BEGIN
ROLLBACK TRANSACTION AddBook <.>
PRINT'A book with the same name already exists'
END
ELSE
BEGIN
COMMIT TRANSACTION AddBook <.>
PRINT'Now book added successfully'
END
----
<.> Not explained in the course, but this is akin to programming, we can declare
variables in an SQL statement, this declares an `int` variable named @BookCount
<.> And here we start logic by marking it under a transaction named `AddBook`
<.> We then run some insert and check after the insert if we get **two rows with the
same name**, meaning there actually already existed another record with that same
name (for that same book)
<.> In case yes, we already had that book in the database, we rollback the named
transaction, and also print a nice message.
<.> In case no, we inserted the first record for that book, then we commit the
transaction cementing it to be written to desk, on top of writing a nice message

So yeah, I can definitely see the use cases here, it's weird that we don't use
these more often though, good leveraging of transactions can save you a lot of
headaches to be fair.

**SAVEPOINT statements**

- `SAVEPOINT` is used to temporarily save a transaction so that you can roll back
to that point whenever neccesary
	- So, you can choose to scope statements with a transaction, commit them,
	but after a certain point, due to requirements or something else, you are
	not sure of how things will pan out after, hence you choose to mark this
	as a `SAVEPOINT` and based on the result of the next group of statements you
	can either go back to the beginning (where the `SAVEPOINT` started) or continue
	executing forward.
- Savepoint names must be distinct within a given transaction.
- After a savepoint has been created, you can either continue processing, commit
your work, roll back the entire transaction, or roll back the savepoint.
	- So, kinda cool, and I guess savepoints are created always inside `TRANSACTION`
	I guess? then they become like a second layer of restoration points depending
	on the use case and logic.
+
Another verbose example, but good to illustrate what these statements do:
+
[source, sql]
----
UPDATE employees SET salary = 7000 WHERE last_name = 'Banda'
SAVENPOINT banda_sal <.>
UPDATE employees SET salary = 12000 WHERE last_name = 'Greene'
SAVEPOINT greene_sal
SELECT SUM(salary) FROM employees
ROLLBACK TO SAVEPOINT banda_sal <.>
UPDATE employees SET salary = 11000 WHERE last_name = 'Greene' <.>
COMMIT <.>
----
<.> Okay, so a good "formula" so that you don't forget. Is to literally after
some statements ran, you go and do a `SAVEPOINT <name>` you are literally placing
a save file at this point in time for the database's **STATE**
<.> Perhaps after some operations, you realize that numbers don't add up, or you
just made a mistake, so you can invoke the "loading" of the save file with `ROLLBACK
TO SAVEPOINT <name>`. This essentially _rollsback_ all state changes done after
said `SAVEPOINT`. And with that idea in mind you can now run new logic that adheres
to something that's _"correct"_.
<.> As you can see this UPDATE would then differ from the one that was done before
but was rolled back due to it being out of the scope of the specific `SAVEPOINT`
<.> Lastly, you can commit all the changes made up until that point.

This `SAVEPOINT` concept definitely feels nieche, never heard or saw it ever being
used to be honest. But then again, maybe I just wasn't exposed to environments
that leveraged the concepts in interesting or clever ways.

=== Quiz 4: Introduction to SQL Commands

Which of the following is not a type of SQL statement?

- SELECT
- INSERT
- MODIFY [X]
- DELETE

What is the purpose of the SQL statement "INSERT"?

- To insert new records into a table

Which SQL statement is used to modify data in a database?

- UPDATE

What does the SQL statement "SELECT * FROM employees;" do?

- Selects all columns from the "employees" table

What is the purpose of the SQL statement "ALTER TABLE"?

- To modify the structure of an already existing table

Which SQL statement is used to retrieve unique values from a column?

- SELECT DISTINCT [X]
- SELECT UNIQUE
- SELECT UNIQUEVALUES
- SELECT DIFFERENT

What does DDL stand for in SQL?

- DATA DEFINITION LANGUAGE

Which of the following SQL statements is considered a DDL statement?

- SELECT
- INSERT
- CREATE TABLE [X]
- ALTER

Which DDL statement is used to remove a table from the database?

- DROP TABLE [X]
- DELETE TABLE
- REMOVE TABLE
- ERASE TABLE

What is the purpose of the SQL statement "GRANT" in DCL?

- To give privileges to users

What does TCL stand for in SQL?

- Transaction Control Language

Which of the following SQL statements is considered a TCL statement?

- SELECT
- INSERT
- COMMIT [X]
- UPDATE

Which TCL statement is used to roll back a transaction in SQL? 

- ROLLBACK [X]
- UNDO
- CANCEL
- REVERT

What is the function of the SQL statement "SAVEPOINT" in TCL?

- To mark a point in a transaction to which you can later roll back to

What does DML stand for in SQL?

- Data Manipulation Language

What is the purpose of the SQL DELETE statement?

- To remove records from a table

Which of the following statements is part of Data Control Language (DCL)?

- GRANT

Which SQL statement is used to revoke previously granted permissions?

- REVOKE

In SQL, what does the DENY statement do in terms of permissions?

- Revokes permissions from a user or role

What is the purpose of Data Control Language (DCL) in SQL?

- To control access and permissions

== Query and Manipulation of Data using SQL

=== Welcome to Query and Manipulation of Data using SQL commands

We will look at:

- SELECT statements
- Expressions, conditions, and Operators
- WHERE Clause, ORDER BY Clause, GROUP BY Clause
- Select from two or more tables - JOINS
- Different types of JOINS
- What is subquery and how to design and use subquery
- INSERT statement in detail
- UPDATE statement in detail
- DELETE statement in detail
- Difference between DELETE and TRUNCATE statement

=== CREATE TABLE(s) and Temp Table(s)

What is a Table?

Database objects that contain all the data from the database. We can have multiple
tables, and every bit of information/data is stored within the tables.

- Tables are logically organized in a row-column format
- Each row represents a unique record
- Each column represents a field in the record
- MSSQL uses 1024 columns as a max (but 30 are at most applicable to real life)

**CREATE TABLE Statement**

````
CREATE TABLE [database_name.][schema_name.]table_name
(
	pk_column data_type PRIMARY KEY,
	column_1 data_type NOT_NULL,
	column_2 data_type
,
table_constraints);
````
We should specify the name of the database in which the table is created.
We need to specificy the schema to which the table belongs to, if we don't, it
will fallback to `dbo` which is the default schema for any MSSQL Server database.
We need to specify the table name.
Each table should have a primary key consisting of one or more columns. Typically
you list primary keys first and then other fields. (It's not mandatory to have a
primary key, but a good practice is to have a primary key always, and there's disadvantages
to not having one).

As we know a database holds many objects inside of it. Tables, Procedures, Triggers,
Views, and **Schemas**, amongst others.

`SCHEMA` is a logical group of database objects that are related between each other.
It's but a logical collection that we can use to separate ideas.

When declaring a PRIMARY KEY we have to specify it in-line in the case of one column
primary key cases, but if we have to specify more than one column as a primary key
this will have to be done as a table constraint.

- Each column has an associated data type, this is the most important part. We
**need** to specificy the type of value for the column.
- A column can have different constraints such as `NOT NULL` and `UNIQUE`. So the
statements after the `data_type` are called constraints
- `CREATE TABLE` is huge and complex, many options are available with it, and depending
on the use case you can do further research and find all sorts of nieche things.

Here's one example:

[source, sql]
----
CREATE TABLE sales.visits(
	visit_id PRIMARY KEY IDENTITY (1, 1), <.>
	first_name VARCHAR(50) NOT NULL,
	last_name VARCHAR(50) NOT NULL,
	visited_at DATETIME, phone VARCHAR(20),
	store_id INT NOT NULL,
	FOREIGN KEY (store_id) REFERENCES sales.store (store_id) <.>
);
----
<.> An identity column is basically a column that auto-increments on each insert,
you can set the "step" for it also, so that it increases at a rate of 1, or something
else.
<.> Constraint, this states that this table (sales.visits) is connected to `sales.store`
through a `store_id` column that in the other table goes by the name of `store_id`

**What is Temporary (temp) Table**

When we create a table, then that gets immediately saved to disk. It is persisted.

**However,** there are use cases in which we don't want to persist a table, we might
need it temporarily.

- A **temporary table in SQL**, exists only for a limited period of time in the
SQL Server
- A temporary table stores a subset from a normal table for a certain period of
time
- Temporary tables are particularly useful when you have a really large set of
data in a table, and you are constantly interacting with a specific subset of
that table.

E.g., You have a students table, and you only need to do queries for students that
study in class 10. Do you want to query 2 million records everytime? What if we
can just move 2000 records to a smaller table, without saving it in disk. That
is a **temporary table**, that we use for a bit, and once we are done processing
we simply drop this table.

That's how the concept of a temporary table came to be. (You filter data once,
and then leverage it for operations). Just as an extra bit of info that will be
studied later, the `tempdb` is where temporary tables live, but any data stored
within this database is lost on restart. So, it's a bit like an in-memory database.

[source, sql]
----
SELECT BusinessEntityID, firstname, lastname
	into #TempPersonTable
	from [Person].[Person] WHERE title = 'mr.'

CREATE TABLE #TempPersonTable (
	BusinessEntityID int, Firstname nvarchar(50), lastname nvarchar(50))
	INSERT INTO #TempPersonTable
	SELECT BusinessEntityID, FirstName, LastName
	from [Person].[Person]
	WHERE title = 'mr.'
)
----
If you add a hash sign before a table name, you mark that table as a temporary
table (will be saved in tempdb). Meaning this will disappear when server is
restarted (or you can also drop the base manually in the middle of your query)

As you can see there are two ways of creating tables. One is the `SELECT - INTO`
approach, and another one creates a table with `CREATE TABLE` and then inserts
into it with a `INSERT INTO - SELECT`. Either form is okay.

Let's now lab all this theory.

- When you create a table and you want to reference another table with a
FOREIGN KEY constraint, the target table **HAS TO HAVE SAID REFERENCED COLUMN
AS A PRIMARY KEY**. Otherwise this will fail.
- When creating a temporary table, that temporary table lives for **THAT CURRENT
SESSION**, if you were to open a different session and try to create the very same
temp table, you wouldn't get hit with an error, and that is because a different
session has no knowledge of that temp table.
	- Of course, you can DROP that table as any normal table

=== What is a View?

- In SQL => Like a virtual table that contains data from one or multiple tables.
We can look at that table from a perspective. VIEW does not hold data at all,
it doesn't really exist physically in the database.
- It's a collection of properties, extrapolations from different sources of a
database that can be easily queried through a name
- A view has a unique name, you can't have two views with the same name
- It contains a set of SQL queries that are pre-defined and used to fetch data
from the database
- It contains database tables from single or multiple databases

We can create a view with endless possibilities, the sense of direction/logic
should be given by the requirement that is leading us towards writing a VIEW
that retrieves all the necessary data we need.

There are many ways of writing a VIEW:

- A GUI (ew brother)
But the use case in this example is interesting:
What if we wanted to get from a person its personal info, (Title, firstName, lastName)
and the phone number and name (or type) of the phone number.

Okay, the GUI is interesting since it has a pretty nice design with all the primary
needs and ways to shape your view.

The neat part is that everything you click on, and tune with the GUI can generate
an SQL. The GUI actually just generates the SCRIPT. We follow the same (kinda weird
pattern) as before. We can try to close the View editor, and it will prompt us to
save the object. We will accept that and give it a name.

And with a named view stored, we can just do a `SELECT *` on that newly created
view's name. That way you don't have to ultra write all JOINS and statements you
have, already abstracted under a name a specific shape of the data, no need to
write the whole thing yourself when this is stored and easly callable.

=== SELECT Statement in detail

Purpose of a database: Saves information, to retrieve and give it a meaningful form.
And get an insight into what's going on with that data. We make guests, we perform
analysis and make decisions.

And so, **SELECT** is probably the most important thing to understand in RDBMS.

SELECT expressions
FROM table_name
[WHERE clause]
[GROUP BY clause]
[HAVING clause]
[ORDER BY clause]

Code in brackets is **optional**.
Words in **capital letters** are all keywords or _reserved words_.

When we are putting column names after `SELECT` we can specificy which columns
we are going to select. We don't have to list **everything** if we don't need it.

If we want to list all the columns: `SELECT *` this is a _select star_.

_Small note:_ The order of columns doesn't matter at all.

Let's now demo a bit

- We will select things from `AdventureWorks` since this is an already setup database.

[IMPORTANT]
====
MSSQL Server is **NOT CASE SENSITIVE**
====

- Depending on if you are using the respective database, you will get intellisense
and/or errors for queries you try to run
	- There are different ways to select a database, both on GUI or with the `USE`
	SQL statement. Be sure of the context in order to run commands appropriately
- A good practice is to put with capitals all the SQL keywords so there's a visual
separation between information from objects and the languages reserved lingo
- The GUI even tells you how slow a query is, depending on use case, we might
identify a bottleneck based on a query we run
- When we select specific columns and not all of them, queries tend to "accelerate"
- With `*` you get ALL ROWS. But if you want a subset, you can leverage `TOP x`
- There's a button to do a `SELECT TOP 1000` if you want to avoid writing SQL (don't)

=== Operators, Expressions and Conditions

If we want further control over the data, we get access to tons of other features.

**SQL Operators**

- This is the way to refer to the symbols we use to perform mathematical and logical
operations in SQL
- There are three types of SQL Operators

Arithmetic Operators

- Used to perform mathematical calculations for SQL statements

Relational Operators

- Used to find relations between two columns. Meaning to compare the value of
two columns with SQL statements

Logical Operators

- Used to perform logical operations on the given expressions in an SQL statement

Expressions

- An expression is a combination of one or more values, operators and SQL functions
that evaluate to a value
- These SQL expressions are like "formulas" written in query language
- You can use these to query a database for specific data

Types of Expressions

- SQL Boolean Expression: A query that fetches **one result at a time**
`SELECT * FROM dataflair_employee WHERE age = 26;`.
- SQL Numerical Expressions: Used to perform mathematical operations on the stored
data, e.g., We want to get employees whose age multiplied by two is greater than 50,
then select rows.
- SQL Date Expressions: Basically, we leverage dates, and their formats to then
select based on criteria (before a date, after a date). A Date will always have 3
pieces: Year, Month, Date.

**Conditions**

A condition specifies a combination of expressions and logical operators, this
should return a boolean value. (AND, OR). We use them to chain multiple combinations
of expressions plus operators.

=== WHERE Clause, ORDER BY Clause, HAVING BY Clause, GROUP BY Clause

These are really important and critical.

**WHERE Clause**

Query writing **is an art**. And the most important tool is `WHERE` and that's
because it helps us sort the data, and order the data.

- We use this specific clause to specify a condition used to fetch data from a single
table, or perhaps when we join multiple tables.
- If a given condition is satisfied, then it only returns results that match the
condition. So we eliminate data we don't need on the SELECT statement
- This is not used only in SELECT, it's also used in UPDATE, DELETE statements.

**ORDER BY Clause**

- Used to sort the data in ascending or descending order, based on one or more columns.
- MSSQL Server uses ASC order by default

````
SELECT table_name
[WHERE condtion]
[ORDER BY column1, column2, ... columnN][ASC | DESC]
````

The way order will work is to order by each column as if in layers. E.g.,
Order by `column1`, once that's sorted, order by `column2`, and so on...

**GROUP BY Clause**

- One of the most important clauses next to `SELECT` has to be `GROUP BY`
- This is used to arrange identical data into groups

_E.g.,_ We have a table of employees, and we want to find out the number of employees
born on each year from 1992 to 2000.

- We can group the employees by year, and then count all the records on each group.
- Another use case would be like "group customers by country"
- Often we use `GROUP BY` paired up with aggregation functions (COUNT(), MAX(), MIN(), SUM(), AVG())
so that we group the result-set by one or more columns
- `GROUP BY` should always precede `WHERE` in a `SELECT` and it should precede an
`ORDER BY`

````
SELECT column1, column2
FROM table_name
WHERE [conditions]
GROUP BY column1, column2
ORDER BY column1, column2
````

A **really important thing to keep in mind** is that you should have the same
number of columns in the `SELECT` as in the `GROUP BY`

**HAVING Clause**

- `HAVING` is the way we can apply `WHERE`-like logic but with `GROUP BY` result-sets
- The `WHERE` clause places conditions on the selected columns
- Whereas the `HAVING` clause places conditions on groups created by the `GROUP BY`
clause
- The `HAVING` clause must follow the `GROUP BY` and must precede an `ORDER BY`

````
SELECT column1, column2
FROM table1, table2
WHERE [ conditions ]
GROUP BY column1, column2
HAVING [ conditions ]
ORDER BY column1, column2
````

In order to express equality you have:

- `=` Equals
- `!=` or `<>` Not Equals

You can write comments with `--` which are single line comments, and multiple lines.
Comments are pieces of SQL that will be ignored when run. Comments are really useful
to note down data as a notebook, convenient.

When we want to run queries against `varchar` columns, we have to put literals between
single quotation marks `WHERE Class = 'hello'`.

Another selections are made based on `Dates`. So, we have to know how to select
by a date format: `WHERE ModifiedDate >= '2013-11-8 00:00:00.000'`. And you can
also do `<= '2013-11-8 00:00:00.000'`.

When selecting varchar columns, we have the `LIKE` operator that paired up with
wildcard notation: `LIKE 'MAT%'` will attempt to partial match the column values
on a basis of **starts with**, **ends with**. Depending on the position of the `%`
sign, then the partial match will correspondingly match.

A way to aggregate all the column values of a table can be done with `MAX` which
will return the max value of all the records that are being selected. By default
the names of these aggregated columns are not meaningful, but if you use **alias**,
you can do something like `MAX(Rate) as MaxRate`.

_Complex Queries:_ Are actually just queries that combine more conditions through
`AND` and `OR` statements. The results satisfy more and more conditions that get
aggregated. Making the selection more and more specific.

Depending on what you want, you can select records that satisfy two conditions
at once, or that only satisfy specific conditions. Query writing _is an art_.

If you want to select multiple values you can make use of lists and the `IN`
statement. Depending on the type of column this could be an equivalent to something
like `AND Id > 500` => `IN (500, 501, 502)` or not

[NOTE]
====
Remember, that `null` is an actual value, it's not "empty".
====

You can equal no `null` by doing queries with clauses such as `IS NULL` and `IS NOT NULL`

**You can refine your queries as much as you want, it's solely dependly on your skills
and the understanding of the database architecture you have in hand**.

A way to "measure" the quality of a query is that it gets results optimally, and
with the minimum amount of verbosity.

When operating with dates, sometimes you don't want to write the whole literal string,
so you can leverage some functions:

````
SELECT * FROM [HumanResources].[EmployeeHistory] WHERE Year(ModifiedDate) >= '2014' ORDER BY ModifiedDate DESC
````

This will simply compare the year specifically, if you want to work with months
you can leverage `Month()`.

**NOTE:** When using aggregator functions paired up with `GROUP BY` the aggregators
are "ignored" from having to be stated inside the `GROUP BY`. But any other column
has to match.

`GROUP BY` when being used with multiple columns, behaves the same as `ORDER BY`,
it will make groups, and inside those groups, other groups will be applied.

If you are using `HAVING BY`, be sure that you have that column that is being
operated under the `GROUP BY`.

=== Select from two tables - JOINS

Why do we need to select from two or more tables?

- Can't we dump all the data on a spreadsheet and sort things there?
R: Yes we can, but it would be incredibly time-consuming, tedious, and prone to
errors.

The **real** power of SQL, however, comes from working with data from multiple tables
at once.

- Relational databases are designed to be joined
- Each table in the database contains data of a specific form or function

_E.g., Some data might have info on a customer, different dimensions to him,
and then we might have another table that's larger that stores the transactions for
this customer. Price, quantity, different qualities_. Now, a customer can have thousand
of transactions (E.g., Amazon, Ebay). The universe of things that you can buy is
in theory limitless, so a lot of records per purchase are bound to appear, we
wouldn't want to repeat customer info alongside the transaction info, would we?
Having too much overlapping data between tables is **wasteful and can negatively
impact system performance**.

- But that doesn't mean we don't care about the linkage or relationship between
each table.
-- Analysis performed on a single table, generally is not that insightful, it's
when we query multiple tables that we truly get true insight into the data that's
being stored in a database.

That's where `JOINS` come into the picture, and help up connect information from
different tables.

**How JOIN works**

- When we join two tables, we are linking them together via a selected characteristic.

Generally, you will join tables on IDs, Primary Keys that turn into foreign keys
when placed in foreign tables. _Why IDs?_ Because they should be truly unique.
If we try to join on a characteristic that is shared by more than one record,
the database will not be able to truly connect records since there will be a
clash and wrong info might result from a select.

_Hence we should join on the basis of IDs that are unique_.

- Let's create a table (Employee)
- Let's select the table
- Let's insert some test rows there
- We'll join on the ID basis
- We'll join on the EmpName basis

We can see that if we try to join a non-unique column, **we get wrong data** and
that's because the database can't know for sure to which of the respective row
we want to connect to, so overall, we don't get "errors" but we get data that is
just not true.

_Tips:_

- If there are columns with the same name on both tables, you have to be specific
-- You can put alias to the tables like `a`, `b` to access them more easily. Now
that doesn't mean you should put awful alias names, always alias them to something
short but that it make sense (initials of each word is a good starting point)

We can then apply all the clauses that we saw before, it's building on top of
the clauses, conditions, groupings, havings, and then adding that layer to selects
with JOINS.

=== Different types of JOINS

- INNER JOIN
- LEFT JOIN
- RIGHT JOIN
- FULL JOIN

**INNER JOIN**

- This selects all rows from both tables as long as the condition is satisfied.
- Default join when we don't specify anything with the `JOIN` keyword. (i.e.,
if we write `JOIN` it will be taken as an `INNER JOIN`)

Syntax:

````
SELECT table1.column1, table1.column2, table2.column1
FROM table1 INNER JOIN table2
ON table1.matching_column = table2_matching_column
````

**LEFT JOIN**

- This returns all the rows on the left side of the join which have matches on
the right side table.
- The rows for which there is no matching row on the right side, will contain simply
`null`.
- This join is also known as `LEFT OUTER JOIN`

Syntax:

````
SELECT table1.column1, table1.column2, table2.column1
FROM table1 LEFT INNER JOIN table2
ON table1.matching_column = table2_matching_column
````

**RIGHT JOIN**

- Similar to LEFT JOIN, but the difference is that it returns all the rows that
are on the _right side of the join_ with their matches on the left side table's
rows
- The rows that have no matches will be filled with `null`
- This join is also known as `RIGHT OUTER JOIN`

Syntax:

````
SELECT table1.column1, table1.column2, table2.column1
FROM table1 LEFT INNER JOIN table2
ON table1.matching_column = table2_matching_column
````

**FULL JOIN**

- This is a combination of a `LEFT JOIN` and a `RIGHT JOIN`
- Result-set will contain all the rows from both tables
- The rows for which there is no match will be filled with `null`

Syntax:

````
SELECT table1.column1, table1.column2, table2.column1
FROM table1 FULL JOIN table2
ON table1.matching_column = table2_matching_column
````

When making joins, if you use the `SELECT *` syntax it will output all the columns
from both tables. But you can just select specific columns of any of the two tables.

You need to understand how you are writing a query, and then apply the different JOIN
that is necessary in order to get the data you want. Depending on what side of the
JOIN a table is you will need to change between LEFT or RIGHT JOIN. Even something
such as a FULL JOIN can be useful in specific use cases, but it all depends on you
figuring out which join will get you the result you are looking for.

[NOTE]
====
Understand that JOIN at its core is simply a way of connecting two tables based
on a specific column's value. These connection columns should be unique, otherwise
clashes might throw our selected records into disarray.
====

=== What is a Sub Query?

- Inner query, or Nested query is a query embedded within another query, specifically
in the `WHERE` clause.
- Used to return data that will be used in the main query as a condition to further
restrict the data to be retrieved
- Subqueries are not restricted to `SELECT` statements, they can also be used within
all manner of other clause types, alongside all manner of operators

Subqueries are useful in scenarios that we have to get a value from the database
for each specific row and then apply some sort of filtering condition. A JOIN
will aim at outputting **multiple records** whereas a susbquery is executed for
each specific row, in order to SELECT some value from the database based on said
row, and then apply filtering.

**Rules to follow**

- Subqueries must be enclosed within the parenthesis
- Commonly, a subquery can only select **ONE COLUMN**, unless we have multiple
columns to compare to (from the main query) we should always limit it to one
- We **cannot** use an ORDER BY in a subquery (but the main query can have it)
- Subqueries that return more than one record, can only be used with `multiple value
operators` such as `IN`
- We cannot use `BETWEEN` within a subquery

SUBQUERY:

````
SELECT column-names
	FROM table_name
	WHERE value IN (SELECT column-name
	FROM table_name2
	WHERE condition)
````

SELECT ID
FROM Customer
WHERE ID IN (SELECT ID FROM Customer
WHERE SALARY > 4500)

==== About Schemas

A side note, nonetheless, schemas are a very interesting concept within databases
that are used for different reasons, in the case of the AdventureWorks database
it uses schemas to separate by sub-domain different tables.

It should make easier for us looking up, and understanding how a data model is
architected.

_Since the default schema is `dbo` if you don't specificy it, queries will try
to use tables in said schema_.

Also, if you **fully qualify** the table name, you will inevitably run into
using a database and a schema within it.

==== Practical example notes

When working with subqueries, I think the best way to simplify it is by saying that
we are simply selecting from a table, and then running checks against each row by
doing a second query, the way to connect should be (most of the time) by selecting
the primary key (ID) of the main query's table with the sub query's table, and in
the subquery run the specific conditions and extra checks you want to apply to this
other table that somehow relates to the specific record at hand.

=== INSERT Statement in detail

Probably the most important DML statement in SQL.

- It let's you add one or more rows to a table or a view in a SQL Server database
- Primary DML statement out there, along DELETE, MERGE, UPDATE
- You can use INSERT to add data that you specifically define
- You can also INSERT data that you select from another table or view

**Basic Insert Statement**

- In a basic INSERT statement you must specify the name of the target table and
the data values you want to insert to a table
- INSERT INTO table_column (column_1, column_2, ... column_n) VALUES (value_1, value_2, value_3)
- You can also do multiple inserts tied into one clause:

````
INSERT INTO table_name VALUES (1, 'John', 34), (2, 'Doe', 33), (3, 'Morgan', 32)
````

Depending on the order for the column names, then the corresponding values should
align.

**Identity column**. These columns should be autogenerated by the database, so
when inserting you can skip declaring the Identity column (usually an ID) and
just insert everything else. The database will still pick up on the insert event
and autogenerate the respective value. If you have setup default values for a
column it will also insert the default value.

Sometimes, you don't want to insert things manually either, you can then SELECT
data from a table or a stored procedure/view, and then dump it into a table.

[NOTE]
====
Sometimes we get requests to update a large table, or some other operation that
might be a bit risky and it might take a long time. We can take a back up of a
table in that case.
====

````
SELECT * INTO marketing.customers FROM sales.customers
````

This statement will create a new table, and all of its structure inspired from
the source table, and all of its records will be copied over as well.

The inserting from a `SELECT` is something I never saw before, and it's pretty
cool, cuz it opens a lot of options to make backups, or to just slice away data
from a table and parse it onto another table:

````
INSERT NameOnlyTable (FirstName, LastName)
SELECT FirstName, LastName From SalesStaff WHERE StaffID > 3
````

See how this can open up endless possibilities since we can apply all the other
operators we have learned so far, so we can slice data with tons of freedom.

````
SELECT * INTO SalesStaff_bkb FROM SalesStaff
````

And this query can be run **without having a `SalesStaff_bkb`** table created.
Such a query is really powerful and really handy. Again _I never saw this in
theory or used practically_. But knowing you can do something like this is so
useful and just, **_cool_**.

=== UPDATE statement in detail

SQL `UPDATE` is used to modify existing records in a table. It's the one of the
primary modification.
- An `UPDATE` always should contain a `SET` and we can update multiple rows
at a time.
- We can use `WHERE` to determine specificity of the `UPDATE`
- We can use the `FROM` clause to identify tables or views

````
UPDATE table_name SET column1=value1, column2=value2, columnN=valueN WHERE condition
````

E.g., `UPDATE SalesStaff SET SQuota = 500000`, `UPDATE SalesStaff SET SQuota = SQuota + 50000`,
`UPDATE SalesStaff SET SQuota = SQuota + 50000, SYTD = 0, SLastYear = SLastYear * 1.05`

These multiple examples show how you can update **all rows** from a table, or apply
a sort of _"sum logic"_, as well as how we can update multiple columns at a time
(from all rows).

But how can you be more focused? I.e., How can you update only a specific set
of rows, (in most use cases we want to limit the number of rows we update).

- To limit the rows we are going to update by using the `WHERE` clause. The scope
of the `UPDATE` immediately gets reduced by applying conditions

````
UPDATE SalesStaff SET TerritoryName = 'UK' WHERE TerritoryName = 'United Kingdom'
````

Sometimes we want to actually **retrieve data from other tables** and use those
selected values to update the **target table**.

E.g., `UPDATE SalesStaff SET SalesQuota = sp.SalesQuota FROM SalesStaff ss INNER
JOIN Sales.vSalesPerson sp ON ss.FullName = (sp.FirstName + '' + sp.LastName)`

As you can see, we can combine `UPDATE` with a `SELECT` that has been joined,
it's a really interesting way of combining different clauses to then fulfill
specific use cases that we have in mind.

[NOTE]
====
We will look into views later, but we can literally select specific columns from
a view and then create a table plus inserting those selected column values into
a new table
====

[source, sql]
----
SELECT
FirstName + ' ' + LastName,
TerritoryName,
TerritoryGroup,
SalesQuota,
SalesYTD,
SalesLastYear,
INTO SalesStaff
FROM Sales.vSalesPerson
----

_NOTE:_ Notice how there's even a naming convention here with `vXXX`. Prefixing
a `v` should let anyone reading the name know that this is a **VIEW**.

_NOTE 2:_ Using this `SELECT INTO` is so useful to create test tables or backups
on the fly, so please make use of it. It's really cool.

Another thing worthy of note, `JOIN` is _flexible_. Meaning that it can join not
just on `ID`s it can also join on "random" combinations and so on:

[source, sql]
----
UPDATE SalesStaff SET SalesQuota = sp.SalesQuota
FROM SalesStaff ss JOIN Sales.vSalesPerson sp ON ss.FullName = sp.FirstName + ' ' + sp.LastName
----
Notice how we are basically building the `FullName` value on the right side of the join,
since we want to feed info from the `vSalesPerson` view, yet we don't have a direct
way of joining by an ID (because we didn't clone an ID in the first place). But this
just goes to show that even if mistakes were made, or things weren't taken into account
at first, you can still salvage what you can by being a bit more creative.

=== DELETE statement in detail

DELETE is probably one of the most used clauses of the DML, the most important thing
to keep in mind is. **ALWAYS REMEMBER TO SPECIFY WHERE CLAUSES**. Always use DELETE
statements within transactions.

That way, the desired result will be achieved. _Be really careful with DELETE,
UPDATE_. Check the query multiple times, run queries on lower environments, verify
the results are the expected ones, and **THEN** run the `DELETE` on PROD.

_Basic DELETE statement_

`DELETE table_name`

This statement deletes **all the records** in this table. You can then add an optional
`FROM` => `DELETE FROM table_name`. The result is the **same**.

_Note:_ `UPDATE` works at a **column level**, however on `DELETE` we work at a
**row level**.

The `WHERE` clause can be used to add constraints to the `DELETE` we are trying to
run, that way we delete a **subset** of rows from a table. Another way to slice
a dataset is by combining the `TOP` clause with `DELETE`

````
DELETE TOP(20) PERCENT FROM SalesStaff
````

This is an example of taking the first 20 records of the SalesStaff table and then
deleting them. Statements such as this one are seldomly used in _production_, since
most of the time there data is sensitive and we prefer to use `WHERE` to pinpoint exactly
the records that should be deleted. But on lower environments, we tend to run things
such as `DELETE` mixed-in with `TOP`

Sometimes we run DELETEs that leverage subqueries:

````
DELETE SalesStaff WHERE StaffID IN (SELECT BusinessEntityID FROM Sales.vSalesPerson
WHERE SalesLastYear = 0)
````

[NOTE]
====
Loopup tables are special types of tables that help us reference data that might
live in other tables or other databases but with good _locality_
====

With lookup tables and `JOIN` statements combined we can also do DELETEs that are
really specialized:

````
DELETE SalesStaff FROM Sales.vSalesPerson sp INNER JOIN dbo.SalesStaff ss ON sp.
BusinessEntityID = ss.StaffID WHERE sp.SalesLastYear = 0
````

As you can see we are simply going to do an `INNER JOIN` with another table based
on an specific column, and we are then filtering based on a criteria that revolves
around `SalesLastYear`.

_NOTE:_ The job of a DBA is not to create tables all the time, but it's imperative
to **know** the syntax, so at the beginning of one's learning journey it's good
to write all queries yourself to familiarize yourself with the syntax

We can lab tons of the statement we have seen so far, but here's a really interesting
use case:

[source, sql]
----
BEGIN TRAN <.>
DELETE FROM SalesStaff WHERE CountryRegion = 'United States' <.>

COMMIT <.>
ROLLBACK TRAN <.>
----
<.> You can think of this as a **marker**, the moment you run this line, all the
following SQL statements will be bubbled inside of this transaction.
<.> Just as an example we are encasing inside a transaction our `DELETE` statement
<.> Whenever we are inside of a transaction and we hit a `COMMIT` then all the
SQL statements that are inside the transaction are written to disk, in other words,
**persisted**.
<.> If we weren't to write a `COMMIT` we can always perform a `ROLLBACK TRAN`, in
essence if we have a running transaction before this statement, this will take all
the statements that ran and will effectively **undo them**. That way, we get these
checkpoints that we can "rollback" into.

Transactions keep **in memory** any clauses that we choose to encase.

The ultimate way to `DELETE` that is weird is this:

[source, sql]
----
DELETE SalesStaff
FROM Sales.vSalesPerson sp <.>
INNER JOIN SalesStaff ss <.>
ON sp.BusinessEntityID = ss.StaffID
WHERE sp.SalesLastYear = 0 <.>
----
<.> As you can see, right after a `DELETE table_name` we are literally tacking on
a `FROM` that is the initial table.
<.> We then perform a "normal" `JOIN` as we would with another table, but notice
here how this right side is actually the original `DELETE table_name` table.
<.> Lastly, we can also apply `WHERE` clauses to slice and specifiy a specific
subset rows within this `JOIN`

This syntax is definitely something I haven't seen at all, or at least not as
often, so it's nice to know it exists, it definitely looks strange at first,
but I'm guessing if you keep using it and share within a team, I am **sure** that
this can become widely used, you just need to _get used to it_.

=== Difference between DELETE and Truncate Statement

- `TRUNCATE` is used to delete complete data from an existing table
- This does basically the same as `DELETE`, but it can't take `WHERE`, it can't
be rolled back (because this doesn't get put in the database log file)
- `TRUNCATE table_name`

_DELETE vs TRUNCATE_

- DELETE can be really target, TRUNCATE will delete a whole table
- DELETE is DML, and TRUNCATE is DDL
- DELETE will delete one row at a time, and log each transaction, TRUNCATE deallocates
the data pages directly, the deallocation is the one thing that gets logged (a sort
of batch operation)
- DELETE is slower than TRUNCATE
- For DELETE you need at least a DELETE permission, for TRUNCATE you need ALTER
- _This is really important_ If you DELETE rows from a table, `Identity` columns
will retain their sequence on subsequent inserts, however `TRUNCATE` takes care
of resetting the sequence number

**PAGE:** Is the basic unit of storage when it comes to databases.

=== What is a Stored Procedure (SP)?

**Really important programming concept.**

- We can "save" queries on the database, and just pass parameters to said query,
and we get the results. (Instead of passing a whole query over the network).
- A stored procedure is a batch of statements which are logically grouped and
stored in the database
- Stored Procedure is a type of code in SQL that can be stored for future use and
can be used many times without modification
- Instead of calling a query you can literally just call a stored procedure
- You can pass parameters to a stored procedure so that its behavior can be dynamic

_Advantages:_

- **Better Performance:** The procedure calls are quick and efficient, they are
compiled once and run cached. Every time we run a query an **execution plan** is created,
however with SP, once we store and compile it, the SP gets saved in an executable
form that is reused. _We reuse the same execution plan_
- **Reduce Network Traffic:** Over the network we only pass a procedure name, instead
of a whole query, calling an SP is _very simple_
- **Reusable:** Multiple users, and clients can execute the SPs, so we can access
query data without conflicting with other users, besides, because this is more
performant, multiple people using it won't be as strainous for the server
- **Security:** We eliminate direct access to tables, and we can even encrypt the
source code of stored procedures, we obfuscate table names or other data that
might be an attack vector
- **It can be easily modified:** We can easily modify the code in the SP without
having to restart services or applications. One challenge that can occur when dealing
with apps is that if we have to change query logic that lives at the application
level we have to re-compile, re-deploy the whole application layer. However with
an SP we can change the logic inside the stored procedure, circumventing all that
work for app, that way the database becomes the point with said logic

````
CREATE PROCEDURE procedure_name
AS
sql_statement
GO;
````

- First you have to `CREATE` a procedure, but then in order to run it, we have to
**CALL IT**: `EXEC procedure_name`

_Parameters:_

````
CREATE PROCEDURE procedure_name @param1 type, @param2 type
AS
sql_statement
GO
````

- And when you want to execute a procedure with parameters:

````
EXEC procedure_name param1, param2;
````

I.e. `EXEC SelectAllPersonAddress @city = 'New York'`, or `EXEC SelectAllPersonAddress 'New York'`

- There are **two types** of stored procedures _system stored procedures_ and
_user stored procedures_. Whenever you create a procedure this will be created
**under** the _user stored procedures_.
- You can `DROP` a procedure, and then create a new modified version or you can
directly `ALTER` a procedure.

Calling a **stored procedure** makes our life easier, instead of writing a whole
query that can actually be _very complex_ in nature.

[source, sql]
----
CREATE PROCEDURE SelectAllPersonAddress @city = 'New York' <.>
AS

BEGIN <.>
SELECT * FROM Person.Address
WHERE City = @city
END <.>
----
<.> As you can see you can even declare default parameters, in case the user
doesn't provide anything, so that we fallback to a default value
<.> We can mark the chunks of logic with `BEGIN`
<.> We can mark the end of a chunk of logic with `END`

_Note:_ Parameters are not mandatory to use them, but it makes no sense to declare
a parameter you are not using _so bear that in mind_

[NOTE]
====
It's not really recommendable to encrypt SPs, even though the feature is there,
it's cumbersome and you need **a really good reason** to encrypt it. Otherwise,
just avoid it.
====

=== What is a function?

Another programmatic concept in SQL.

**A set of SQL statements that are stored in the database**, we call the function to
perform a task. There are differences with a _stored procedure however_.

**Functions cannot mutate data, they can only return data (selects)**.

- Why should you write functions? Instead of writing really big SQL tasks, we can
abstract that complexity behind a function and then just call it (maybe pass some
parameters) and then get the expected result.
- We cannot call a stored procedure in a `SELECT, INSERT, UPDATE, DELETE`. However
we **CAN** call a `Function`.

````
CREATE FUNCTION [database_name.]function_name [parameters]
	RETURNS data_type AS
	BEGIN
	SQL statements
	RETURN value
	END;
````

- We have **two types** of functions in SQL Server:
	- Built-in functions
		- These are ready-to-use utilitary functions that we can use however we
		see fit, they are also separated by domain-use case:
			- **Scalar Functions:** Functions that operate on a single value and
			return a single value. E.g., `upper('dotnet'), lower('DOTNET'), convert(int, 15.56)`
			- **Aggregate Functions:** These functions operate on a collection of
			values and return **ONE VALUE** E.g., `max(), min(), avg(), count()`
			- **Date and Time Functions:** Related to date and time. E.g., `GETDATE(),
			Datediff(), DateAdd(), Day(), Month(), Year()`. _Note: There are **tons**
			of functions that we can use, trying to cover all of them is not plausible,
			but just now that the docs are your friends, and you'll be surprised by
			what type of functions/things are already available for you to use,
			so always try to research and try stuff out.
	- User-defined functions
		- Sometimes there are needs/requirements for domain-specific functions,
		things that aren't available to us out-of-the-box, hence we are forced
		to go and write our own custom **user-defined functions**. These are not
		restricted to a _user defined database_.
			- **Scalar Function** These functions return a **single value** as
			a result of all the actions performed by the function
			- **Inline Table-Valued Function** This will return a **table variable**
			as a result of actions performed by the function
			- **Multi-Statement Table-Valued Function** This is just like the previous
			type, however the specific difference is that a **table variable must
			be explicitly declared**, on top of defined whose value can be derived
			from multiple SQL statements

_**Difference between Functions and Stored Procedures**_

- The function must return a value but in **Stored Procedure** it is optional.
Even a procedure can return zero or n values
- Functions can have only input parameters for it whereas Procedures can have input
or output parameters
- Functions can be called from Procedure whereas Procedures cannot be called
from a Function
- Function **ONLY** accept `SELECT`
- `PROCEDURES` cannot be used in a `SELECT`
- `PROCEDURES` cannot be used in `WHERE`, `HAVING`
- An exception can be handled with a try-catch block in a **Procedure** whereas
**Functions** _do not have this feature_
- We can use Transactions with **Procedures**

==== Labbing

Basically the functions that we can use, open a window into calculations and operations
with the rows we select from a database:

[source, sql]
----
SELECT MAX(Rate), MIN(Rate), COUNT(*), SUM(Rate)
FROM Employees
----

These are **aggregate functions**, basically from all selected rows, they will
process the specific column values and spit out **one value**.

We can even use functions such as: `print 'dotnet', print UPPER('dotnet'), print CONVERT(int 15.56), print GETDATE()`.

And many others, (you can even chain together functions: `print DAY(GETDATE())`.
You can even do stuff like: `MONTH()`, `YEAR()`.

Now, if we create a custom function:

[source, sql]
----
CREATE FUNCTION GetFullName(@firstName VARCHAR(50), @lastName VARCHAR(50))
RETURNS VARCHAR(101)
AS
BEGIN
RETURN (SELECT @firstName + ' ' + @lastName)
END
----

And then you can call this built-in function [by prefixing the schema always (e.g.,
dbo.function_name)].

````
SELECT dbo.GetFullName(firstName, lastName) as FullName, Salary FROM EmployeeTestTable
````

As you can see, we can use the function "in-line" in a way that processes columns
of a row, and then the spitting-out result be its own "column", and then also
combine that with other columns from a table that we might be _selecting_.

_Table-valued functions_

This type of functions is basically us returning a SELECT or some table structure
resulting from the operations that run in the function:

[source, sql]
----
CREATE FUNCTION fnGetEmployee()
RETURNS TABLE
AS
RETURN (SELECT * FROM Employee)

SELECT * FROM dbo.fnGetEmployee() <.>
----
<.> For whatever reason the tutorial doesn't call the function with parenthesis,
however (_at least in MSSQL 2022_) we do need to call it like this.

_Note:_ As an extra, depending on the `RETURNS` type, the functions will be grouped
under different groups in the `Programability` folder of the database.

_Note 2:_ Notice how we are prefixing the functions with `fn` this is a great
convention to keep in mind so that you have order in the functions you create
and stuffs.

_Multi-statement table-valued functions_

[source, sql]
----
CREATE FUNCTION fnGetMulEmployee()
RETURNS @Emp TABLE <.>
(
	EmpId INT, <.>
	FirstName VARCHAR(50),
	Salary INT
)
AS
BEGIN
	INSERT INTO @Emp SELECT e.EmpId, e.Name, 5000, FROM Employee e
	UPDATE @Emp SET Salary = 20000 WHERE EmpId = 1 <.>
RETURN <.>
END

SELECT * FROM fnGetMulEmployee() <.>
----
<.> Notice how we are declaring a `TABLE` with a variable name
<.> And followed to the `TABLE` statement we are declaring all the columns that
this table will hold
<.> And this is the **biggest difference between multi-statement and the simple
table-value functions, and that is we are indeed calling `UPDATE` and even `INSERT`,
but we are calling those statements for the table variable, **not for any of the
database tables**, if we tried to act on the former, we would run into an error.
<.> This is another special syntax to keep in mind, we don't explicitly return anything,
because we declared the `TABLE` variable at the beginning it is _implicitly understood_
that said table variable will be returned by the end
<.> Lastly, as mentioned on the previous example, we need to call the function
with parenthesis, and also notice how we are doing a `SELECT * FROM`, _this is
because we are returning a table in the end_, so all the statements, and conventions
we know for database tables are applicable to this _table-variable_

Function is a great programming objects, and are used _a lot in production environments_
to make life easier for a DBA, since he can just call functions instead of building
from scratch always complex SQL statements and stuff like that.

=== What is a Trigger?

- These are special kinds of stored procedures that are stored and executed on
the database object, database and server events.
	- Stored procedure can be called by the user through an `exec` or with a
	job, it will return a value and perform stuff
	- However a **trigger** can get automatically executed, _as soon as an event
	happens_
	- The easiest to illustrate use case is this: Imagine we want to keep audit of
	changes to the salary of employees (i.e., we want to know **who changed the salary**),
	we can create a trigger to be sitting waiting for any changes on the `Employee`
	table, when this event is detected, the `Trigger` can then in another audit
	table insert a new record with information in regards to _which user performed
	this change of an employee's salary_.
- There are **two types of triggers**
	- Data Manipulation Language (DML) triggers which basically are running  in
	response to `INSERT, UPDATE, DELETE` statements
	- Data Definition Language (DTL) triggres are running in response to `CREATE,
	ALTER, DROP` statements. We can also run triggers that react in response to
	some system stored procedures that perform DDL-like operations
	- **Logon** triggers which fire in response to **LOGON** events
- The syntax of creation of a `TRIGGER` is designed to be built around the firing
of events
+
[source, sql]
----
CREATE TRIGGER [schema_name.]trigger_name
	ON table_name
	AFTER {[INSERT].[UPDATE].[DELETE]} <.>
	[NOT FOR REPLICATION] <.>
	AS
	{sql_statements}
----
<.> As you can see, we attach to a table **and** then we specify which type of
event on the table this trigger will react to
<.> This is a special kind of flag that whenever a table is replicated we can take
the triggers it has along (or not)

Here's an example of an easy trigger:

[source, sql]
----
CREATE TRIGGER AfterInsertTrigger ON TriggerDemo_Parent
	AFTER INSERT <.>
	AS
	INSERT INTO TriggerDemo_History VALUES <.>
	(
		SELECT TOP 1 FROM TriggerDemo_Parent,
		'Insert'
	)
	GO <.>
----
<.> You can replace this as `INSERT, UPDATE, DELETE` this is an `INSERT` example
<.> This trigger simply inserts a new value into a history table, hence this
statement is focused around that sole fact. Notice how we are doing a `SELECT TOP 1`
which will select the most recent inserted row and all of that will be dumped into
a column of the history table
<.> A good practice is to always leave a `GO` by the end of statements

_Where can you check out registered triggers?_

You should head down to a **table**, and open up the `Triggers` folder under it.
(On MSSQL Studio). This reinforces the concept that triggers are attached to
a table, and react based on events on it.

[NOTE]
====
You can also disable triggers, which basically is a _soft delete_ of it. Since,
even though it will be stored and attached to a table, it won't run as usual.
====

````
CREATE OR ALTER TRIGGER EmployeeInsert <.>
	ON Employee
	AFTER INSERT
AS
BEGIN
	INSERT INTO Employee_History VALUES
	(
		(SELECT MAX(EmpID) FROM Employee), <.>
		'INSERT'
	)
END
GO
````
<.> This syntax with `CREATE OR ALTER` is pretty handy since it turns your creation
into an idempotent statement, in case the trigger already exists, we will just overwrite
its definition.
<.> In here we simply want to know the newest ID that was inserted, the SQL statement,
here returns a scalar value. This reinforces the idea that the trigger runs **AFTER**
the specific event we described (in this instance after the _INSERT_)

[NOTE]
====
Something mentioned in the course is that a DBA is **not** in charge of writing
triggers and stored procedures, this is `the task of a database developer`, which
really got me thinking in the layers on top of layers of specialization that some
organizations manage. I guess I will follow a bit of the _"theory"_ that **VALVE**
uses. If you over-specialize, you put barriers on people that keep them from actually
being truly productive. So you need to strike the perfect balance between the
"number of roles" your employee should take so that he's truly comfortable and
productive, instead of slumped with politics and bureaoucracy.
====

Besides my tirade, the advice is to know the concept, the basics, and know how
to read these database objects. The DBA **will have to sometimes step-in**, and
at the very leat _understand_ how to manage an SP, Trigger. On top of troubleshooting
it.

=== MS SQL Server INDEX

Any RDBS actually leverages this concept of **INDEX**, so it's good to understand
it as a principle, and less as a feature of MS SQL Server. What you need to understand
as the fundamental, most important piece of information is that we use indeces,
to **boost performance**

_Why do we need an INDEX and what is a Table Scan?_

Let's look at an analogy:

----
_Imagine you are reading a book about the human body. Each chapter focuses on
one part of it, the reproductive system, the nervous system, the digestive
system, etc. How would you find information about the **heart**? if you didn't
have an index as **all books have**. You would have to read the entire book
by yourself, or at the very least, ready just about enough so that you can pinpoint
where is it that the information about the heart is located_
----

This is, indeed, **time consuming**.

Now imagine we have a **table** and it _has no index_. That by definition is called
**A HEAP**, a table which is stored with no _underlying order_.

When rows are are inserted into a heap, there is no guarantee as to where the
pages will be written, if they are written in the same order. It can be in
position 3, and then it will be in position 5.

And so, with `HEAP`s the database engine _has to do a TABLE SCAN_, it reads the
whole Table looking for a specific result, once it finds what it's looking for it
then results, but still _it scans the whole thing_.

- _Small dataset, means that a full table scan is fast_.
- What if you have 5 million rows? Slowness creeps in.

**INDEX:** This comes to the rescue, since you basically declare specific data
to become _easily queryable_. Without an INDEX, SQL Server would have to go through
ALL the rows. That is a **table scan**, which is something you don't want to do
in a large table.

- You can create an index on a view or a table
- Indeces are created **on columns**, and the idea is that an index should help
you look way faster data on those specific columns
- If you create an index on a primary key and then search for a row based on
that specific key, SQL Server will first lookup in the INDEX, and _then_ after
locating it, it will **then try to look _with the index_ in the table for the
specific row**
- You can create indeces on most columns in a table or a view
- The exceptions are primarily those columns with large object (LOB) data types,
such as image, text, and varchar(max) (_specifically **max**_)

**Structure of an INDEX**

Basically, _and this is a structure that can you get if you read
https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321[Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems],
called a **B-Tree**.

The easiest way to understand it:

- You have a series of nodes, (hence a tree), that is hierarchical, comes from a
parent one (root) and separates in two children, those children then get separated
in their own **two children** and then their children **also have their own two
children**. And each node holds a small _sub-set_ of values. Example:

- Node 1: IDs from 1 - 200
- Node 2 (Node 1's Child): IDs from 201 - 250
- Node 3 (Node 1's Child): IDs from 251 - 300
- And so on...

If we are looking for a specific `ID` we will then start jumping from one node
to the other, once we are within the _range_, we will then read the node and find
the specific _value_.

The **database query engine** will go down from the root to the _leafs_, jumping
and jumping in this _binary-structure_. It's quite efficient, specifically since
the **deeper you go**, the **more granular the nodes become**. So you can be
sure that you can find the object you are looking for in a timely manner, since
the structure is _cleverly designed_.

=== Clustered, Non Clustered Indexes and Index Design Considerations

**Clustered Index:**

- When we use this type of index, then the values are all stored at the leaf
level of the index
- The _entire row of data_ associated with the primary key value of 123 would be
stored in that **leaf node**
- **An important characteristic** is that the indexed values are sorted in ascending
or descending order
- _There can only be **one clustered index** on a table or a view_
- It's **after we attach a clustered index to a table** that all records there
will become sorted, otherwise they will be ordered as they are inserted

**Non-clustered Index:**

- The **index values and a pointer to the actual row data** are the only things
that are stored in the _leaf nodes_
- This means that the query engine must take an additional step in order to locate
the actual data
- The pointer's behavior varies depending on what the table that's holding the
referenced row is, (heap or clustered), if it's a clustered table, then the pointer
will talk to the `clustered index` to then navigate towards the correct row,
if it's a `heap` then the pointer will point directly to the **actual row**. Which
means that depending on the nature of the table, things will be slightly faster,
or slower
- We **cannot** sort non-clustered indeces
- SQL Server Supports up to 999 nonclustered indeces
- But you shouldn't push to get 999 nonclustered indeces created. They can be
helpful but also detrimental for performance.

_If the indeces are not properly created, then your `INSERT`, `UPDATE`, `DELETE`
might end up affected_.

- You can attach nonclustered indeces on a table or view, and you can also put
an index on **multiple columns**

==== Index types based on configuration

**Composite Index:** Really common type, these are indeces that have more than
one column attached to them. (_You can put up to 16 columns on an index_), and
also this index should not exceed the 900-byte limit. Both nonclustered and
clustered can be composite indeces.

**Unique Index:** This is a special type with the added characteristic of enforcing
the uniqueness of the values on the index, if you use a composite for this,
then it will combine all the values as **one** and then check that composed value
is `unique`, individual values can be repeated in this case.

_Note:_ When we create a `primary key` or a `unique` column, **we automatically
create a `UNIQUE INDEX` on said column, and based around the respective column.

**Primary Key:** When you create a _primary key_ on a table, then automatically
that key becomes a `clustered index` based around that specific column.

**Unique:** The moment we attach a `UNIQUE` constraint to a column, then a _nonclustered
index_ based around that column is attached to the respective table.

**Covering index:** Basically, if you have a specific query, that uses, let's say,
**three columns**, `firstName`, `lastName`, `salary`. Then we can create a really
specialized query that **covers** those three columns, the query will then leverage
_this specific index_ to make its operation faster.

==== Index Design Considerations

_Too many indeces, or bad practices can cause tons of PROBLEMS_

- Indeces take up considerable **disk space**, so you don't want to be going around
creating an index for every single thing
- Indeces are automatically updated the moment the **linked columns** are also
updated, this means it can lead to _additional overhead_ on write operations
- If you have a table that is _heavily updated_ and **you have to add an index
to it** then add the least amount of columns possible to said index, that way
you lessen the chance of having to rebuild the index on every **write**. Also
**DON'T OVER-INDEX YOUR TABLES**
- On small tables putting an index is actually less performant than a simple
`Table Scan`.
- For clustered indeces, try to keep the length of the index columns as small
as possible, don't make it bigger than necessary
- Try to implement indeces on columns that **do not take nulls**, a `null` can
easily throw indeces into disarray
- When possible try to implement **unique indeces**
- For **composite indeces**, a good `trick` is to order the columns that will
be queried by a _query_ on the same order that said columns are queried.
- When building queries, specially `writes` try to write them efficiently, don't
do small queries one at a time, try to use queries that write things _in a batch
manner, with one statement_
- Create nonclustered indeces on columns used frequently in statement's predicates
and join conditions (so WHERE, JOIN)

````
CREATE INDEX index1 ON schema1.table1(column1)

CREATE CLUSTERED INDEX index1 ON database1.schema1.table1 (column1)

CREATE UNIQUE INDEX index1 ON schema1.table1. (column1 DESC, column2 ASC, column3 DESC)
````

=== Index Fragmentation and Lab for Index

- Index Fragmentation is a common source of database degradation.
- Fragmentation is when there's a lot of spaces inside of a data page (internal
fragmentation)
- When the logical order of pages in the index doesn't match the physical order
of pages in the data file (external fragmentation)
- Fragmentation-related performance issues are most often observed when executing
queries that perform index scans
- Queries that perform index seeks way not be affected by _high index fragmentation_
- The index fragmentation is the index performance value in percentage, this can
be fetched by the SQL Server DMV (Dynamic Management Views), or you can also use
the GUI. _You basically have the ability to get info on how fragmented your index
is_.

_In short:_ If you have a data page, and you try to insert data into it, and it
has empty space, then that **new piece of data** will take place in the empty page,
but then when reading the data on said page you will have data that is no longer
sequenced, it's spread across different pages and it bears no logical order, not
time-wise, nor domain-wise

**Internal Index Fragmentation**

- This occurs when data pages have _too much free space_
- SQL Server inserts data into 8 KB pages. So when you insert data that is less
than this, then the page used for that insertion is left with some space blank
	- If you insert more data for 8 KB, then the remaining info will be sent to
	_another data page_, now of course, that remaining piece of data might not
	fit perfectly the subsequent 8 KB page, so you will again run into a page
	that has some blank space in it
- When you delete rows, then all the info in data pages from those rows gets
also "freed" which is _another possible source of fragmentation_.
- Internal fragmentation causes performance issues when there's an **index scan**,
why, because even though there's essentially _blank spaces_ in a data page, even though
it holds no data of value, it still has to navigate the whole page
	- Performance comes to a halt when SQL Server has to scan many partially
	filled pages for the data it's looking for
- The way to understand why it is more _costly_ is simple. Partially filled pages,
can amount to say **eight pages**. An index scan would have to read 64 KBs worth
of data pages, instead of said **four pages**, that would be able to hold the same
amount of data, but without being fragmented

**External Fragmentation**

- This happens we insert or update data that causes for original data that used
to be on the same data page to be split apart, since it can't no longer fit within
that specific amount of pages, and the rest of info allocated in another data page
that might in a completely different memory sector (end of the storage)
- Random I/O is created, and SQL Server takes more time, since data is no longer
sequentially stored, it's split apart, and so it needs to read more memory sectors
in order to put together a full row's pieces of information

_How do you fix/prevent fragmentation:_

- **You can't prevent fragmentation to a 100% degree**, however you can indeed
do what you can
- Before trying anything, assess the degree of fragmentation and **then** try to
attack it from different angles
- Use the **Dynamic Management Views**, they give us insight into the workings
of the SQL Server, **one of them** is `sys.dm_db_index_physical_stats`. This tells
us for every index, the level of fragmentation. **Once you know the level of
fragmentation**, that's when you can start attacking it:

- **Rebuild:** Rebuild indeces when fragmentation reaches greater than 30 percent
	- This will lock a table, so **be careful the time at which you try to rebuild
	an index**. It's best to do it in _off-hours_
- **Reorganize:** Reorganize indeces between 11 - 30 fragmentation. Way faster.
- **Ignore:** Fragmentation that's at 10% or below should pose no threat to performance

[NOTE]
====
So an index, is something attached to **a table** as we said, so, you can expand
a _table_ on a database, and look under the **Indexes** folder, and you will see
listed all manner of indexes that are attached to said table, you get access to
scripting any of them, and you can also use the GUI to say _drop the index_. It's
also worth noting that even though you might see lots of weird flags, and settings
when scripting a `CREATE TO` for example, you can delete most of that, and the values
will take on **default values**, still, keeping the SQL statement for creation of
an index as simple as possible is way more readable and helpful
====

**You can disable an index**. You can just right click and disable said index.
_What happens here?_ When a query is getting executed, then the index will be
**ignored**

[IMPORTANT]
====
Don't forget, you can only create **ONE CLUSTERED INDEX** on a table. It's only
**non-clustered** indexes that are able to be attached multiple times to a table.
====

So indexes should follow a naming convention to help you navigate them more easily,
that is something like this: `idx_firstName` or `idx_firstName_lastName`.

Secondly, you should be aware of the difference between **an indexed column**
and an **included column**. You would base an index around **one column**, and
you can then include other columns within that same index.

_Note:_ All the GUI created indexes can be scripted, **and I would recommend you
rather use that**.

_Note 2:_ Instead of dropping and re-creating an index you can edit an existing one
with the GUI and say, _include other columns_ and so on.

When you want to start analyzing _fragmentation_ on an index, you just right-click
an index and then head to `Properties`, in there a section for `Fragmentation`
should be available, and you should see all the information you need, on top
of the concepts we already saw:

- **Leaf level rows**
- **Maximum row size**
- **Minimum row size**
- **Pages**
- **Page fullness**
- **Total fragmentation**

That's _easy_ for querying, say ONE index, and stuffs. But if you want the more
practical, faster approach, you will have to rely on DMV. Which is basically
something you can query.

[source, sql]
----
SELECT
    s.name AS schema_name,
    o.name AS table_name,
    i.name AS index_name,
    i.type_desc AS index_type,
    ips.avg_fragmentation_in_percent, <.>
    ips.page_count <.>
FROM sys.dm_db_index_physical_stats(
        DB_ID(),      -- current database
        NULL,         -- all objects
        NULL,         -- all indexes
        NULL,         -- all partitions
        'LIMITED'     -- fast, recommended for regular checks
    ) ips
JOIN sys.indexes i
    ON ips.object_id = i.object_id
   AND ips.index_id = i.index_id
JOIN sys.objects o
    ON i.object_id = o.object_id
JOIN sys.schemas s
    ON o.schema_id = s.schema_id
WHERE o.type = 'U'             -- user tables only
  AND ips.page_count > 100     -- ignore very small indexes
ORDER BY ips.avg_fragmentation_in_percent DESC;
----
<.> The fragmentation percentage is a key value that should help you take decisions
as to where to start solving possible performance issues
<.> But don't blindly just follow the fragmentation percent, because if you have
just a couple or even **a pair** of pages holding all the information of the index,
then there's no real point of trying to "optimize it", so always look at the two
values, if you have _a lot of rows_ and the _fragmentation percentage is high_.
**Then you can start analyzing said index further, and yes, start with optimization,
cleanup and so on**.

Say we saw 39532 pages, and with 50% percentage. What should we do?

**REBUILD**, the fragmentation factor is _really high_. And this is really easy,
just `Right Click` the index and select **Rebuild**. You can `Script` or run
the rebuild through `GUI`.

_It is after a rebuild of an index_, that you should see the fragmentation factor
**DROPPING**. As to _why you should not mind indexes that have high fragmentation
but low page count_ is because even when you rebuild the index, then the fragmentation
factor will not drop at all. It's a fight against the current.

Also, if you look at other type of fragmentation levels, say ones below `30%` then
you can `Right Click` and choose `Reorganize`, after a refresh, _you should see
the fragmentation factor dropping also_.

=== Quiz 5: Query and Data Manipulation of Data using SQL

Which SQL operator is used for pattern matching in a WHERE clause?
- `LIKE`

What is the result of the expression 5 + NULL in SQL?
- 5 + NULL

Which SQL function is used to find the highest value in a column?
- MAX()

What is the purpose of the SQL operator "IS NULL"?
- Checks if a column has a NULL value

Which SQL clause is used to filter the results of a SELECT statement based on a specified condition?
- WHERE

In a SQL query, what does the HAVING clause work with?
- Aggregate functions

Which SQL keyword is used to rename a column in the result set of a SELECT statement?
- As

What type of join returns all rows from both tables, with NULL values where there is no match?
- FULL OUTER JOIN

In a SQL JOIN operation, what is the purpose of the "ON" keyword?
- Specifies the columns to be joined

Which type of subquery returns only one value and is used with operators that require a single value?
- Scalar subquery

In a SQL subquery, what is the purpose of the "EXISTS" keyword?
- Check if the subquery returns any rows

What is a trigger in SQL?
- A rule that automatically executes in response to certain events

What is the primary purpose of the SQL function "COUNT"?
- To count the number of rows in a result set

What is the main difference between a stored function and a stored procedure? 
- Stored functions must return a value, while stored procedures do not

What is the purpose of parameters in a stored procedure?
- To pass values in and out of the procedure

== Microsoft SQL Database Administration and System databases

=== Welcome to Microsoft SQL Database Administration and System Databases

This is a section focused on the usage of Microsoft Server Management Studio and
the different ways we can manage the database with it

=== Overview of MSSQL Management Tools

We cherry-picked some options when installing MSSQL, there are tons of tools,
extensions and things that specialize the type of work and features we take from
this database solution, each of them can be their own world, we will have a look
at what are the possibilities with the whole suite.

**MSSQL Management Tools**

- `SQL Server Management Studio` This is the GUI we use to create databases,
run queries, select data. This is an IDE to access, configure, manage and
develop components of the SQL Server. It lets people from all skill levels to
use SQL Server. You access SQL Server and you manipulate data, you write your own
code. It's probably **the most important tool when working with MSSQL**
- `SQL Server Configuration Manager` This is a tool that focuses more around
changing protocols in clients, swapping service accounts, server protocols, restarting
and even client aliases. You can do some of these operations with _SSMS_ but this
tool has more features and is way more **specialized**
- `SQL Server Profiler` This is a GUI solution that you can use to monitor the queries
and traces in order to measure performance, if you have to do troubleshooting,
you can debug queries that are coming into the database, where it's getting stuck
and stuff like that. This is a much easier GUI that you can use to perform these
tasks than _SSMS_
- `Database Engine Tuning Advisor` is a tool that helps create **optimal** sets
of indexes, indexed views, and partitions
	- These features of SQL are not _as easy as ABMs_, and in critical queries we
	have to create views, indexes, partitions, _tasks really focused on performance_,
	most of the time we have a query, and using our knowledge of how the database
	works we can tune said query. But there are _specialized use cases in which it's
	not so simple, so we rely on a tool such as this one_. It literally identifies
	for a SQL query what type of index, or thing will boost its performance
- `SQL Server Data Tools` provide an IDE for building solutions for the Business
Intelligence components: Analysis Service, Reporting Services, and Integration
Services. To create a report from a report server, or an SSI package using an
integration service, we can do most types of tasks with `SSMS` but the **proper**
tools to do these things are under `SQL Server Data Tools`. Up until SQL Server
2012 they actually came together with MSSQL Server, but they are now separated,
and you have to choose to install it together if you choose so.
- `Data Quality Client` talks to the `DQS` or _Data Quality Services_ that help
maintain the quality of data by enabling multiple tasks, but that are mostly based
around **data cleansing operations**, you can also in turn monitor various aspects
of how things are looking as these cleansing operations run
- `Connectivity Components` are installed to communicate clients and servers,
as well as network libraries for DB-Library, ODBC, and OLEC DB
	- DB-Library stands for Database Library, which is an API for accessing Microsoft
	SQL Server.
	- ODBC stands for Open Database Connectivity, a standard API for accessing
	database management systems
	- OLEC DB stands for Object Linking and Embedding, Database is designed to
	provide access to a variety of data sources, _both relational and non-relational_
	- All of these are **drivers** that help communicate across databases

**Note:** The lesson is imparted with MSSQL 16, and so some assumptions and explanations
are rooted into that specific setup. Like the fact that there are two folders that
get created with this version:

- Microsoft SQL Server 2008: This is probably for backwards-compatibility, in
here there's yet not much of anything is inside of this folder
- Microsoft SQL Server 2016: Under this folder you get **tons** of programs that are
available, all the reporting, engine, and tons of other tools are here.

Say you open the SQL Server Profiler, this is a program with which you can create
**Traces**, you can connect to the SQL Server, and then just run the trace based on
specific credentials and for specific events/things. Building traces can be
_an art of itself_ since you can fine-tune as much as you want and filter as
much as you want of information, you can literally start running queries, or
just using the database and then see `logged` in the profiler all of thes actions.

There's _too much information_ however at times, hence becoming a profiler artist
comes in handy to filter out useless pieces of info you don't need, and focusing
on the real insightful data that will help you understand better about how the
database is behaving. You can in here see the duration of queries, and many
other insights.

[IMPORTANT]
====
If you run traces it will take a toll on the performance of the SQL Server, so
be careful, it's recommended to run that on a second instance, and only query
the specific objects you are interested in, besides, once you are done with your
debugging, you **should shut the profiler down**
====

Another tool we have is **SQL Server Import and Export**, this focuses around exporting
data from the **database** into `other formats`. These can be csv, or other RDBMS
(Oracle, MySQL), it's interesting how there are all these tools that are _highly
specialized_ for specific tasks.

_Note:_ Actually, it is with all the previously mentioned **drivers** that we
get access to do all these operations to transform data, they will connect to
the database instance, extract data, transform it, and then export it in a format
you see fit [E.g., To a CSV it will even give you all sorts of wizard screens to
fine tune how you want the output to look like, what data to extract (you can do
a full database dump, or a table dump, or even write a query to dump)], the tool
is so cool it can even give you previews of how the output will look like. Once
you run that process **once** you can even save this whole operation as an
`SSIS Package` which can be saved to a database. The reason to do this is that
then this whole export process becomes a sort of **object** within MSSQL, you can
attach jobs to it, so that they run the SSIS package, for example, making things
way more easier for you.

[IMPORTANT]
====
What is the job of a DBA? It is not just monitoring databases, the health and
stuffs. It's also ensuring that all jobs, all tasks, whatever might have schedules
and are neede to be running, **do in fact run when they should (daily, monthly, yearly**)
====

_Another good word of advice:_ If you have to make something more than one time,
**create a job/tool that does it for you**, **automates it for you**. _Less-prone
to errors, because when doing things manually, you CAN indeed mess up_.

**SQL Server Installation Center** You can re-use the installation center we used
for installing SQL Server for the _first time_, you can perhaps perform maintenance,
upgrade something, install more features, configuration checking. It's a tool that
is there for you to use, _however_, be aware that sometimes you **need the installation
media for specific tasks, not _everything is possible_ but a lot IS**.

_NOTE:_ Apparently the job of a DBA, _is mostly based around being extremely proficient
at using SSMS_, so the focus _at least of this course_ will be around it, but it's
good to still now that **there's a sea of tools and possibilities out there as well**

=== Exploring SQL Server Management Studio (SSMS)

This is an application developed by **Microsoft themselves**, that way you can
administer other the components from MSSQL. It's a centralized management platform
for any SQL Server requirements. It's a suite, aimed at letting you do amazing
things with SQL Server.

- Creating jobs
- Debugging
- Reports
- Querying
- Create databases, tables, authorizations, permissions

_You can do anything that you can do with SQL Server_, it's an all rounder tool/application.

. **Connect** to a remote/local server, SSIS, SSRS and Analysis Service
. Explore server properties and its objects
. Explore databases and its objects (management)
. New Query / Query Parsing / Execution / Results of Execution Plans
. Monitor SQL Server using Activity Monitor / sp_who2 / SQL Logs
. DBA Admin tasks (backups, Logins, Jobs, Maintenance Plans, etc)

**Let's lab**

[NOTE]
====
Didn't know abouts this, so when you open up SSMS the first window that pops-up
in order to set to which database to connect, actually under the `Server type`
dropdown will show you other services you can connect to (other _servers_), **that's
how you actually access these other features, or well _at least that's one way to
access them with SSMS_
====

When selecting a database server to connect to, it's not just _write a name_ you
can also use the **Browse** window, which allows you to see a tree view of servers
that might be already saved, and even a `Network` window can be used to _automatically_
find all SQL Servers that might be discovered in the network the machine is. If
we are connected in a **domain** and other SQL Server instances are in said domain,
then they get _auto-discovered_. So, the _write the name_ is actually the `last resort`.

_Most of the time, we rather use Windows Authentication to use the studio_.

_You can right click a server and then run reports on it_. There are **already**
built-in reports that you can just run with the click of a button.

_Note:_ Stuff like memory consumption, top transactions by lock count, and so on.

_Note 2:_ There's apparently `powershell` integration with MSSQL Server apparently.
That's _kinda crazy_.

If you right-click the server icon and then click on `Properties` you get all sorts
of windows with all sorts of information as well, really easily available. **But**,
you need to be aware of something **really important**. _All of the information that
you see on the GUI, is actually queryable, there's a query that can spit out the
same information_ (E.g., `print @@servername`)

_Another example:_ You can go into a `Memory` tab, and then adjust how much memory
is a specific database going to use. But, the real quicker, is that as many knobs
you turn, as many inputs you fill, you can alwys press the `SQL` button at the top,
and all of that "setting" can be put in a query form. _And let's not forget about
the power of text-based format/code/data and how it gets along **so well with
versioning**_. You can literally run a `query` that can adjust the memory of
a database.

- You can set the server to **only be accesible with Windows Authentication or
a hybrid**
- You can also set logs to capture successfull **and** failed logins
- There's also sections for compliance purposes, that help when you want to
keep your database _auditable_

And interesting concept comes into the front the more you start poking and playing
around with all the knobs and switches SSMS. This is:

- Server Settings: These are settings that go beyond just the Database service,
they might deal with filesystem concerns, logging concerns, compressing concerns
even.
- Database Settings: These are all really domain-specific to the database service.

For example, in **compression**, you can literally flip a switch so that any backup
that you make (after you apply the setting) is a compressed backup, the reason as
to _why you would want that_ it really depends on you **brother**. But most of the
time, really big databases, that are backed up frequently, or sent over to somewhere
else, _compressing is a great solution to avoid spending too much time_.

_You can even configure the default paths to which Data, Logs, Backups will be saved
to_. Why do we have this setting here? Because, you can change whatever defaults
are here, and the next time you create a database, it will take **these settings**
as the base for where it will store its things. (Really quick example: You install
a new drive with more storage, and you want everything now getting saved to
that new drive, _it's always about your need and requirement_)

**Important yet complex**

Under the _Advanced_ section, you get access to these settings:

- Cost Threshold for Parallelism = Default 5, one of the most missed configurations
when looking at performance for SQL Server. This means that when SQL estimates the
**cost of a query**, (_the unit of cost in databases, is always **time**, since
it's critical_). If a _query takes too much time_. We refer to that as **it is
a costly operation, it is a costly query**. Now what does the **5** mean? That
whenever SQL Server analyzes a query, tries to build an execution plan, the moment
it determines a query will take longer than **5 seconds**, it will automatically split
the query into parallel streams in an effort to make the query _return faster_.
Perhaps from longer than 5 seconds, the query can run in 1 second.
- Degree of parallelism = This depends how many CPU cores are employed to run
the split queries in parallel and so on. SQL Server is smart and will scan that
and set a default value. But you can definitely **fine tune everything**

After that, there are tons of features that SSMS has, you can script the **database**
for instance, which really isn't table data nor even structure. But the barebones
database setup.

Besides that:

- You can take a database offline = No one can connect to it, it's live, still running
but cut off from communication
- You can do a backup
- You can do a restore
- Import Data
- Export Data

**Security:** Place to create new users, and assign rights to them. (The tab within
one specific database). There's also a _Security_ tab but at a server level.
Which means that you have logins at these two different **levels**, so don't confuse
a database user with a server user. This layer of granularity is pretty handy since
you can really build distributions of permissions for each database, and for each
user.

[IMPORTANT]
====
The _right way to do creations, inserts, updates_ is **not with the GUI**, it has
to be with queries. Not really practical
====

_What is the purpose of a query?_ We want to select, insert, update, delete, truncate
**something**.

_SQL Server, nor any programming language cannot think for themselves._ We need to
specify **exactly what we want**, right name, right syntax. This is called _**query
parsing**_. Whenver you are trying to run a query, the IDE, and the database then
after will **tell you if you have an error**. So there's this extra functionality,
that SSMS offers us to **literally select a query that we have written and run it
through query parsing**.

This will _not actually run the query_, it will just check if it's a valid query.
The SQL Parser is the one that checks and then notifies if the query is valid
or not.

When it comes to **selecting** data, SSMS offers a million options of how we can
transform that selected data into simple clipboard table-row content, perhaps we
also add the headers, or we export that into a CSV file. _Or you can even just generate
SQL Reports through the **reporting service**_. The sky is the limit and you get
to choose what to use and how.

Okay, I **had no idea this was a thing**, and it's so useful. Say we are working with
huge queries, that are like 200 lines or even bigger. If there's a syntax error
somewhere the error message states almost always will state something like `Syntax
error on LINE X`. Now it's easy to pinpoint that with small queries, but what if
we had the same line numbers that we have on coding editors/IDEs? You can enable
that pretty easily by going to:

- Tools > Options > Text Editor > Transact-SQL > General > Line Numbers (checkbox)

If you enable it, on the query editor windows you will now get to see line numbers
which just makes life like _so much easier_ when it comes to pinpointing perhaps
and error _on some line_.

_Extra note:_ A great QoL upgrade (I'll admit as much) is that if you run multiple
queries that return multiple results, then all the outputs stack respectively.

_Extra note 2:_ You can also choose how you want the output of selects, in the
typical grid fashion, or full on text, and stuffs.

=== Exploring SQL Server Management Studio (SSMS) (part - 2)

**Execution Plans:** When we write any query in MSSQL Server, then an execution
plan is always generated, it can boil down to a graphical representation of
_everything that the query will do_, it's the `query optimizer` in charge of
that. Now, it's in the name, whenever we run a query, that query is run by the
`optimizer` and he makes sure that the query is configured/setup in a way that
takes **the least amount of time**. And in so, an _execution plan is created_,
and this lays out **all the operations that will take place in order to get
our results from the query**

If you want to visualize the execution plans it's literally a switch/click of
a button before you run queries. In the pannel up above search for an option 
(to the right of the `Execute` button) that says "Include actual execution plan".
This should be highlighted and after highlighting if we run a query, we **should
see a new tab** next to the result and message tabs with a diagram, that you
can click, hover over and see all sorts of statistics about the execution plan.

A quick test can be run by running a query as simple as this:

[source, sql]
----
SELECT * FROM Employee <.>

SELECT * FROM Employee
WHERE EmpID = 79 <.>
----
<.> This does a `Clustered Index Scan`, a `scan` is literally going through
**all the records**
<.> This does a `Clustered Index Seek`, you know which row of a group of rows
is the one you want and you display only **that one**

If you run both queries together, you will see that each of their respective
execution plans **are visible**, the info stated up above is just what the
execution plans stated

_There's a lot of info you can extract from the execution plans_. For example
on the `Seek vs Scan` operations, you can see how many rows were read, and
one of them will only read **one** instead of the whole table's rows.

When writing queries, it's extremely important to see `execution plans` as these
sort of tools and extra bits of info that help us troubleshoot and understand
better what our queries will do. **WHY?** Because that way if we have a query that's
slow or that's presenting problems now, _we can then see exactly **what is it**
that it's the bottleneck_

Just like the thing we used for the _query parsing_, we have the same idea in this
**realm of execution plans**. We can choose to "not run, but map out an execution plan
for a query", it's more of an _estimation_, since the query optimizer _will try
its best by looking at all the information it has on that table/indexes/and so on_
and spit out **what it thinks it will cost**. And that will differ from the option
we ran before which actually draws the execution plan alongside the result of the
run of said query.

_Small Note:_ You can choose to turn on-off Intellisense (I don't know why you would
want to turn off this key feature, but off you go)

`View > Object Explorer Details` is another type of view you can use to see the
different databases, tables, information of schemas and other stuff on them. It's
a _"different way of viewing all the objects that the database has"_. You can right
click the header of the default view and you can add _new columns_ you choose what
info you want displayed, some interesting options are: `Data space used (KB)`,
`Row count`, `Owner`, you can even see the creation dates for tables and databases.
You can even **copy the table info into an excel sheet**, it definitely has _tons
of features_. A really easy-and-quick way to get all of this information.

The idea (_for a DBA_) is to know about the tools he has, to automate as much as
he can, and leverage all the tools and processes that already there figured out,
in place with that very same idea in mind (making things easier, info gathering
faster), **why?** because that will impact the velocity and productivity of
your team, and in turn **the business**.

_Monitor SQL Server_

[IMPORTANT]
====
There are hundreds of things that you can do with the tools that you have on
MSSQL, if we were to talk about _every single feature and detail_ it could take
weeks if not months.
====

Something you have to interiorize as a learner in tech, and in this case MSSQL,
is that you have to also do your own research, _do your homework_ most courses,
and most things in general will introduce you just about enough so you can move
around in the tools, environments that you are trying to learn. They show you
the ropes, but it's after that initial push that it's on you to find and delve
deeper into the things you are interested in/are being asked of you. But there
is just so much information, specially on things like MSSQL Server, it has more
than 30 hours at this point, so most veteran DBAs and people still don't know
all of its extent to 100%. So don't expect or think you need to _now it all_,
cuz it's probably not worth the effort, nor _humanly possible_.

**NOTE:** You don't have to memorize everything, the skill you need is to be
able to research, and teach yourself what you need.

A **really cool pannel** is the `Activity Monitor`, that you can easily access
by: `Right Click on the root of the tree view on the left panel > Activity Monitor`.
You should be able to see to the right a whole screen with some graphs, but also
many sections that give you a better understanding as to what processes are running,
most expensive queries, and stuff akin to that. A **control panel** that really
quickly should help you understand the current health of the database, and perhaps
identify issues that are lurking there (or that are active at that moment)

_Note:_ If a query is really expensive, and it's running a long time, we can see
it logged there in that very same control panel.

[NOTE]
====
Really useful command to see active connections `sp_who2 active`. This will show
in a table view all the active connections, (most of them will be BACKGROUND)
connections that the server uses for its own internal processes, but you can easily
make out the active connections from people like _us_ who are trying to query the
database and so on. If you remove `active` you will get to see more connections
perhaps, most of which might be stale, or leftover from previous connections,
some are just _dormant_ though, they come back online when the time is right.
====

A **really useful feature** is the screen to visualze the MSSQL Server logs,
just right click the main Database seed from the tree view, and expand the
`Management > SQL Server Logs` options.

These are all files, (that live in the folder that we setup initially for `LOG`),
yet SSMS offers a neat interface that reads them and presents them in a nice
useful UI.

_You can recycle logs, that way you free up space_, you can configure auto-recycle
in MSSQL Server, that way upon your own parameters, after a specific period of time
logs get deleted.

**Backups:** Always needed, necessary. Just right click a database `Tasks > Backup`,
You can even setup "Maintenance Plans" that through a GUI you can configure and
automate backups.

You can also setup **Jobs** that run on a scheduled time, so that you automate
really repetitive tasks with the database.

=== Exploring SQL Server Configuration Manager

This is another feature that gets installed with the default settings. This manages
the services associated with SQL Server, we can also configure network protocols
that SQL Server uses, as well as the network connectivity configuration that the
SQL clients use.

_Basically,_ The SQL Configuration Manager, is a GUI used to setup the services
which are associated to SQL Server, (logons, automatic startups, start up parameters),
then (TCP/IP, ports used) all of these can be configured from this module.

**You get SQL Server Configuration Manager** by default after you install SQL
Server. Both the `Configuration Manager` and the `SQL Server Management Studio`
uso **Windows Management Instrumentation** (WMI) to interface with the server
and change configurations there.

If you search for (if you are on SQL Server 19 for example), `Sql Server 2019
Configuration manager`, you will get a red icon program showing up, and you should
be able to open the configuration manager from there. This is a screen with a
tree view at the left, in said tree table you can see options like:

- **SQL Server Services:** In here you can see all instances of SQL Server that
are installed in the machine, so if you had more than one instance, you should see
both a `Server` and an `Agent` line item. It is in this screen you can start, stop,
restart the SQL Server and its agent.

[NOTE]
====
A nugget of advice, if you restart/start SQL Server, and it loads up fairly quickly,
we good, but if it stays **stuck** 99% of the time means that there's an error
and you have to troubleshoot it so that the database service can start.
====

If you `Right Click > Properties` any of the SQL Server instances, then you will
see a window with tons of information, on interesting tab is called `Log On`, this
is a really useful pannel if you want to change or granularize the access that
specific accounts have over a database instance.

**Example:** If you want to start separating by different service accounts for
each instance, you can choose to do so by changing the `Log on` and inputting the
account name and the password.

There's also `Service` with metadata about our instance, but with a specific
entry called `Startup Mode`. This can be set as Manual, Automatic. The best option
to pick (specially this is the case when on a server that we know is focused entirely on just
have MSSQL running on the background). It's **recommended** to use `Automatic`, that
way the **moment you start the server**, you won't have go into the configuration
manager to manually install the database server.

[IMPORTANT]
====
Remember that MOST if _not all_ configurations that we apply on a specific point in
time **will not be replicated** or _be seen_ unless we restart the SQL instance
====

**FILESTREAM:** Remember that this is for files that are _outside_.

_Another cool option is_ **Always on high availability**. Literally one checkbox
check, and we have baked in the behavior of:

- Our main instance of SQL Server for whatever reason runs into an issue and
is rendered unable to serve the clients
- We immediately redirect the traffic into another instance that should be
there in the background in order to take place of the leader.

**Startup Parameters:** This tells SQL Server _what is needed to start the server_.

You can work the same way with the `Agent` service, and also, the more you read the options,
the more you realize how there are disabled/grayed-out options that won't be
_that easy to change_ because they might be too critical or something akin to that.

==== SQL Server Network Configuration

Most communications are done with TCP/IP. And you can `Right Click TCP/IP Line Item`.
It will open up a window full of data you can read. _The default port for MSSQL
is 1443_. Hence, we **have to enable this port for inbound and output**, otherwise
no communication will be possible for MSSQL purposes.

[NOTE]
====
We sometimes have to keep in mind **cybersecurity**. That is because default ports
exist _for convention purposes_, that is **public available knowledge**, if a
malicious actor **knows that we are running SQL Server**, he will immediately start
scanning and trying to break through those default ports. So a good measure is to
change said default port, _**This is highly dependent on your circumstances though,
it's not a golden rule, but just a nugget of info to always have in the back of
your head**_. Say, if you are building internal applications that are walled-off
the internet because it's an intranet, _it really doesn't make sense to put that
much effort in that_.
====

Another thing to also keep in mind is the fact that **even if you change the port**,
you will have to restart the server so that said port starts getting used for
MSSQL communications.

=== MSSQL System Databases, DO's and DONT's of System databases

**Types of databases**

- User Databases = Databases created by a user which has the permissions for
database creation, any changes and operations within them **will not affect the
functioning of MSSQL whatsoever**, another name for these types is _user defined
databases_
- System Databases = Databases which are needed for MSSQL Serevr to operate. Unlike
**user databases**, these are **key components of the SQL Server**. They are
key data, metadata that is connected to the specific MSSQL Instance
	- Most of these databases are created **when we install the MSSQL instance**
	- If you lose a system database (e.g., master), then the SQL Server instance
	**will start acting up** or overall, just _not work_. The service **will go
	down**. (_**These databases are critical**_)
	- These databases maintain and collect information about the SQL Server
	service. (login, databases, linked servers, reports)

**MSSQL System Databases**

- **Master:** Core system database that manages the SQL Server instance
- **TempDB:** Temporary database to store temporary tables (#temptable), table
variables, cursors, work tables, row versioning, create or rebuild indexes sorted
in TempDB
- **MSDB:** Primary database to manage the SQL Server Agent configuration
- **Resource:** Hidden, we can't access it, it stores physically all of the SQL
Server system objects
- **Model:** Template database for all user defined databases
- **Distribution:** Primary database to support SQL Server replication

**Do's and Dont's**

- **Data access:** When you are using an object _within_ the system databases,
or feature, Microsoft tends to move around the names or specific behaviors,
so be really careful about **migrations**, because if you move from an old
version to the next _and specially if it's a big jump_, you might run into
issues (there's also a distinction between `discontinued` and `deprecated`
features).
- **Changing objects:** Do not change system objects.
- **New objects:** Do not create new objects in the system database. Create
a user defined database, (e.g., `DB_ADMIN`) and you can save all the scripts,
SPs, Triggers, whatever in there.
- **Sneaking a Peek:** You can always read the code for all the system databases,
for whichever purpose that might be.
- **Dropping objects:** If by _any reason_ you need to drop system databases (auditing,
compliance), you **have to document this decision**, and _be very careful_, because
dropping any of the system objects can have repercussions ranging from really
small details to really critical, potentially catastrophic consequences
- **Backups:** Be sure to have a consistent backup, not only of the `user databases`,
also the `system databases`. _There are scenarios in which your system databases
end up corrupted, and if you backed them up, you can bring the instance back to
life_
- **Scope:** If you have multiple instances running _on a server_, **each instance
will have their own set of system databases**. They will be independent, so any
change on one system database from one instance, **will not affect any of the
other databases**.

=== What is the master Database?

This is the main database required for SQL Server to **function properly**.

- Records all the **system level records**, name, login information, user information,
database information, path to databases. **Everything gets stored in `master`**.
	- A big repository to save all the info about that particular SQL Server
	instance
	- If you have several instances, **each one will have its own master database**
- In SQL Server, _system objects_ are in the `Resources database`, which is a
hidden database, we can't access it as simple users. However the `master` database
routes to the metadata in this hidden database.
	- No one can make a change in `master` unless we know what we are doing, even
	then, Microsoft decided to move really sensitive information into `Resources
	Database`
- **If** for some reason we delete the `master` database or something happens to
its info, the SQL Server instance might not be able to boot-up sometimes
- **Remember that we get generally two files** `mdf` and `ldf` files, which are
database and log files respectively (master.mdf, mastlog.ldf), by default they
have an `autogrow` setting to 10% until disk is full

**Restrictions on master database**

- Adding files or filegroups (master will only have **one file**)
- We can **only do a full backup of a master database**
- We cannot change the `collation` of the `master` database
- `master` is owned only by `sa` (System Administrator), **can't be changed**
- We cannot create a full-text catalog, or full-text index
- We cannot create triggers on system databases
- We cannot drop these system databases
- We cannot drop the `guest` user from these databases
- We cannot enable **data capture** on these databases
- Cannot participate in database mirroring
- We cannot remove primary filegroup, primary data file, or log file
- We cannot rename the database or primary filegroup
- We cannot set the database to OFFLINE, otherwise everything will go down and
we won't we able to access MSSQL Server
- We can't make this database `read-only`

**Recommendations**

- **Always have a `master` database backup**
- Back up `master` as soon as you do any of these operations:
	- Create, modify or drop any database
	- Change server or database configuration values
	- Modify or add logon accounts

You should **NOT** create user objects in the `master` database, otherwise you
will have to make _even more backups_. _A rule of thumb is to make a weekly/daily
backup of master_.

_NOTE:_ Do not set `TRUSTWORTHY` to ON. This is a setting that you can set on
databases to avoid security checks, for `master` never turn it on. It's a vital
database, so the **proper security checks** _should be in place_.

**What to do if the master database becomes unusable or unavailable**

- Restore `master` from a backup
- If you can _actually start the server instance_, you should be able to run a
`restore` from **master**, from a full database backup
- Rebuild **master** completely (this is if the SQL Server instance is not coming
online) (complex, beyond the scope of the course)

You can just `browse` the **System databases**, _as you would with any of the user
databases_. It also has basically all the same structures and features as the
other databases. (However, if you check out its `properties`, you will notice
how it has many options greyed out)

You can even see the physical paths at which the database and log files are stored
on disk.

[NOTE]
====
Do not try to do anything to the `master` database. Don't try to mess with it,
you can do as you please with user databases. If **you have to** make a change
in _master_, test your change, do it in lower environments. So, **don't touch
it in 90% of use cases**
====

If you **search further** when it comes to `master` you will see that under `Tables`
there's only like _4 databases_. Which will immediately lead you to believe:
_Well, if everything about the instance is saved into the `master` instance,
**where is it**?, so few tables, and empty?_.

Well, you would be wrong to be looking for info there, _you have to head to the
Views_ section of the database, as we mentioned before, all the information is
actually **walled-off in the Resources Database**, which is invisible, yet we
can poll all of the data from there **through all these views that `master` has**.
There are actually TONS of views, and you can see TONS of information if you
poke _long enough_

Some really useful views under `master` are:

- sys.databases = Just a list of all the databases that are registered in the instance
- sys.database_files = Actually lots of information about the databases, both from
physical disk paths, but also status
- sys.dm_os_process_memory

_Even the instructor of the course tells us that **even though he has experience**,
every time he logs into an SQL instance and starts working with the `master` database,
the views always teach him something new when working.

_NOTE:_ And so, since it's so big, it is basically a given that you have to learn
things on your own, that you have to try stuff out yourself.

==== Breaking the instance by editing master

We can dig the location of the file for the `master` database, and we can try
to edit its name (the easiest way to break the SQL Instance).

First, stop the SQL Service, otherwise it will have a lock on the file. Then
edit the file name, **then** start the service again, it should take _longer than
usual_ and it should in the end throw an **error**. You can hit the properties of
the instance and see the `Startup Parameters` section of info, you will see how
it's pointing directly to master, **AND** you will see the path to the logs.

(Don't forget you can access this through the Configuration Manager)

If you access the ERRORLOG file, and open it with `notepad` we will see how
it states that it _can't find the master file_. It's what we expect, and you can
see with this really small example how you troubleshoot, read logs and find the
reason for the error. You don't just _stand there/sit there_ not knowing what
to do, you realize that you know where to start looking, you learn to read the
error messages, the error logs. _Never forget about this, **NEVER**_.

=== What is tempdb Database?

Another **critical** database for the correct functioning of the MSSQL Server
instance. It is a **global resource** available to all users that are connected
to the SQL Server instance.

- It holds **lots of user objects that are explicitly created**, temporary tables,
temporary indexes, temporary stored procedures, table variables, tables returned
in table-valued functions, and cursors
- **Internal objects that the database creates:**
	- Work tables to store intermediate results for spools, cursors, sorts, and
	temporary large objects (LOB) storage
	- Work files for has joins or hash aggregator operations
	- Intermediate sort results for operations such as creating or rebuilding
	indexes, or certain GROUP BY, ORDER BY, UNION queries
	- _All of these operations_ land us with results that come as **side effects**,
	results that **aren't the final results** but some intermediate results that
	are later used to **GET TO THE FINAL ONES**
- We also store the **version stores**, which is basically a collection of data
pages that hold the data rows that support features for **row versioning**

[cols="1,1,1,1,1"]
|===
|File | Logical Name | Physical Name | Initial Size | File Growth

| Primary Data
| tempdev
| tempdb.mdf
| 8 MB
| Autogrow by 64 MB until the disk is full

| Secondary data files
| temp#
| tempdb_mssql_#.ndf
| 8 MB
| Autogrow by 64 MB until the disk is full

| Log
| mastlog
| mastlog.ldf
|
| Autogrow by 64 MB to a maximum of 2 terabytes
|===

- The number of secondary data files depends on the number of (logical) processors
on the machine
- As a general rule, if the number of logical processors is less than or equal
to eight, use the same number of data files as logical processors. (Example, 8
cores, 8 ldf files)
- If the number of logica processors is **greater** than eight, _still use eight
files_
- **tempdb contention**, this is a concept (I'm still yet to know where to see this,
or how to diagnose this), that a query you ran is _"stuch in the tempdb"_, this means
that the bottleneck is the **temp files**. The best course of action is to start
creating _more tempdb files_ so that contention is alliviated. Add _4 more_, and
monitor again, if the query still is ending up affected by `contention` add 4
more. It's trial and error until the query performance reaches an acceptable level
of performance **for your use case**

**Restrictions of tempdb**

In short, the same restrictions that apply to `master`, although it's not just
that, it's **way more restrictive** actually, I'm guessing it's that way since
this is just a database to store intermediary objects, you shouldn't be messing
with this stuff too much, in most use cases we wouldn't have to care that much
about it. All the info in there is volatile, and shouldn't be payed much attention.

==== Optimizing tempdb performance in SQL Server

- The size and physical placement of the tempdb database can directly affect the
performance of a system
- If possible, use `instant file initialization` to improve the performance of
growth operations for data files
	- Basically this is a different "way" of reclaiming space in disk after it
	is deemed that a sector of memory is "free". For _whatever reason_ we first
	reclaim the space by **filling in with zeros** and then writing data,
	but in this mode, we skip the filling in of zeros, and we just **WRITE**
	directly.
- Pre-allocate space for all tempdb files by setting the file size to a large
value enough to accomodate the typical workload in the environment
	- The autogrowth process is **time consuming**, so if we have the files already
	big, or _big enough_ so that we don't incur in autogrowth, we can get some
	quick wins
- Data files should be of equal size within each filegroup, because SQL Server uses
a proportional-fill algorithm that favors allocations in files with more free space
	- This means that SQL Server when looking at _which tempdb file to write_
	it will always look for the file that has more free space, if you have **one
	file that's way bigger than the others** to SQL Server it will look as that
	file **always has more free space**. And it will stop writing to other tempdb
	files, _which will tank performance_
- Set the autogrowth to a bigger number than the defaults (so that we can reduce
the number of autogrow operations)
- Put the tempdb on a fast I/O subsystem
- Put the tempdb in **its own drive**, a disk that is completely separated from
the **user databases**

==== Moving the tempdb database

- Determine the logical file names and their location on the disk
- We need to change the location of _each file_ by using the `ALTER DATABASE`
- Stop and restart the instance of SQL Server
- Verify the file change
- Delete the tempdb.mdf and templog.ldf files from the original location

**In the lab environment** we can just head down to the system tabes and
`Right Click > Properties`, that way we can see the tempdb details, and in there
we will be able to get info on **where** are the tempdb files stored physically
in our disk.

We should also be able to see the option to add more files on the tempdb, and as
part of the lab section of the lesson we did that, we added 2 more temp files,
even though our VM only has `2 processors`. And we should fill in all the details
respectively (although we also changed the size to 128 MB), and we made the previously
`8 MB` files also be 128 MB. It's worth noting that _you can click the `script`
button at the top of the window to see all the queries that will run under
the hood_.

It's definitely a personal preference, but you should be aware that you can either
choose a query or the GUI to make all of these operations.

[NOTE]
====
Also, don't forget that we are doing all of the lab work in **one machine** and
**simulating the different paths for the different databases on different folders**.
But the ideal case is to separate them in **their own drives**
====

_A DBA should be able to do all of these operations, and understand them at a
fundamental level as easy as breathing_

==== Testing tempdb and theory

A quick way to prove the theory we have been learning so far is to create a
temporary table and examine the `Temporary Tables` node under the `tempdb` database:

````
CREATE TABLE #myTempTable (ID INT IDENTITY(1, 1), Stuffing VARCHAR(100))

DROP TABLE #myTempTable
````

Before running the `DROP` you can create the temporary table, and if you refresh
the `tempdb` database, under the `Temporary Tables` folder, you should be able
to see this new table added, (with a _weird name_ to be precise). Even though
you might be connected to another database, or whatever, under the hood, what is
in fact happening is that the table physically lives in the `tempdb` database.

_Remembering:_ Temporal databases will only live as long as the session that
created them lives. So in theory, the moment that client disconnects all temporary
things created by him should also be dropped. **HOWEVER**, in practice, it's best
to drop the temporary things you created after your whole transaction succeeds,
_and even if it fails, you should properly clean up as well_

Let's try to move the `tempdb`, you usually do this when we are running out of disk
space, so we have to now move it to another disk, or computer.

We can use the GUI as we did before to get the files, however we can also use
a query:

[source, sql]
----
SELECT name, physical_name AS CurrentLocation
FROM sys.master_files
WHERE database_id = DB_ID(N'tempdb') <.>
----
<.> This takes a name, and then immediately tries to find the ID corresponding
to that string value

That way we can copy and paste the path for the `tempdb` files, head one level
above, create a new folder `SQL_TEMPDB_NEW` or something like that. And we will
change the pointer to **this new folder** instead of the old one.

There's also a QUERY for that:

[source, sql]
----
ALTER DATABASE tempdb
MODIFY FILE (NAME = tempdev, FILENAME = 'C:\SQL_TEMPDB_NEW\tempdb.mdf') <.>

ALTER DATABASE tempdb
MODIFY FILE (NAME = templog, FILENAME = 'C:\SQL_TEMPDB_NEW\templog.ldf')
----
<.> This will be a base so that we modify all the temporary files (remember there
was one by default and then two more that we created before). **HUGE NOTE:** It's
only the first _non-numbered_ tempdb file that has a `.mdf` extension all subsequent
files have a `.ndf` extension.

After you apply all the `ALTER` queries you might think that the files will be created
in the new path, and everything will be good. Even if we run the same query to
check for the location of the temp files we will see that it now points to
our new location. But **NO**, you have to restart **SQL Server**, the previous files
are still in use, it's only after a restart that SQL Server rewires to the new path,
and indeed **creates the new files as we instructed**. It's after this _restart_
that we can head down to the old path and actually **delete the files** if we
want to. (since they are no longer being used by a process)

=== What is msdb TABLE

- Is a system database, used by **SQL Server Agent** used for schedulig jobs and
alerts. Although other services also make use of it such as SQL Server Management
Studio, Service Broker, Database Mail 
	- SQL Server Engine = Everything that we did up until this point was living
	in this context
	- SQL Server Agent = Jobs, Backups, Restores, Database Mail, Service Broker,
	everything gets saved in `msdb`. SQL Server Agent runs based on this database
- SQL Server automatically maintains a complete online backup-and-restore history
within tables in `msdb`
- By default, `msdb` uses _simple recovery model_. When a database is with this
mode, we don't save all the transactions in the transaction log, however in
_full recovery model_, every single transaction gets **saved**. Means we can
roll-back in time.
- If we are taking backups and using history tables, **it is recommended to set
the `msdb` mode to `full recovery`** also, be aware that if you upgrade or install
a new instance, the default mode will **always** be `simple recovery mode`, or
if we rebuild the system databases with `Setup.exe` even.

**Physical Properties of msdb database**

[cols="1,1,1,1"]
|===
|File | Logical Name | Physical Name | File Growth

|Primary data
|MSDBData
|MSDBData.mdf
|Autogrow by 10 percent until the disk is full

|Log
|MSDBLog
|MSDBLog.ldf
|Autogrow by 10 percent to a maximum of 2 terabytes
|===

**Restrictions of msdb database**

- The default collation of the server will be used by `msdb`
- Can't drop the database
- Can't drop the `guest` user
- Can't participate in database mirroring
- Can't put offline
- Can't be put as READ_ONLY

Like **any database**, there's a concept of _recovery_. And a **good DBA** should
keep backups periodically of MSDB, and in case of really bad failures or errors,
the main idea is to **recover as much data as possible**, better than _no-data_
is _some data_.

**Restoring msdb database**

- Backup msdb
- Get the version of destination server
- Get the version of source server on which the backup was created
- Match the versions of the source and destination servers
- Ensure exclusive access to the database
- Restore msdb

**LAB**

We access `msdb` the same as `master`, the same as `tempdb`, through the `System
databases` tree branch.

In there if you `Right Click > Properties` and head to `File` you get all the options
to modify the behavior of how the data file and the log file behave. And as everything,
we have the option to **GENERATE A SCRIPT**, any action you take in the GUI can
be then put into a QUERY.

- _We can restore this database as any other_

You can head down to `System Tables` and see different tables that are really
interesting and useful if you know what you are doing:

- `dbo.backupset` = This is a log of the backups, any backup you take, you restore
it will be logged here.
- `dbo.sysjobs` = Another table that shows all the SQL jobs registered in the
database.

You can head way below the tree view to the left, until you find the `SQL Server
Agent`. Right Click > Steps. You can see what it does, by default there's but
_one SQL Jobs_.

[NOTE]
====
Image this use case: Every day, we need to run a query to get some data, and email
it to a user. We can leverage all the features that SQL Server offers. We can
create a SQL Server Job so that the querying is automated, and we can then save
that into a file, and send that through mail to the requesting user (SQL Server Mail)
====

We can create a job to perform a task on a daily-basis, on a specific task. The
information (metadata) about the job is on `dbo.sysjobs`

**EXTRA:** You can even get information on the schedule of jobs, running history:
`dbo.sysjobshistory`.

`msdb` has all the information about jobs, backups, restore events. When looking
deeper into it, we will see how to create our own jobs. All info related to them
should live in `msdb`

**Backups tend to be saved as `.bak`**

==== Small lab on restoring msdb

Imagine you delete the sysjobhistory, you have lost all info of what the hell happened
with the jobs. _Imagine we took a backup of msdb before this awful change_.

Now we can restore it following specific steps:

- We need to stop the `SQL Server Agent` because he's constantly querying this
database, we need exclusive rights to it.
- We can double check if the backup is compatible with our server version:
`SELECT @@VERSION`
	- And we can then run a query to scan a file and see the metadata it holds:
	+
	````
	RESTORE HEADERONLY
	FROM DISK = N'C:\SQL_BACKUPS\msdb_beforedelete.bak''
	GO
	````
	+
	We can double check if both the backup and both our SQL Server instance are
	the same version. _Note: We can look at `SoftwareVersionMajor, `SoftwareVersionBuild`,
	`SoftwareVersionMinor`, and match that to whatever `SELECT @@VERSION` spits, if
	they are the same, we can **confirm the restore will be successful since the
	backup and server are compatible**
- We can restore with a `query` but also with a `GUI`.
	- For now we will use the GUI, just `Right Click > Tasks > Restore`, in there
	MSSQL is smart enough to recognize a backup file for it, but if we don't
	have it availble, we can select `Device` and search for it in the file system
	- Also, _even though I shut down the Agent_ the restore was failing because
	apparently the database _was under use_. You can select the second tab of
	the restore window and check the `Close existing sconnections`, that way
	we make sure we get exclusive access to the database, and no one else is
	getting in the way and blocking the restore

After that a simple "RUN" should suffice (DON'T FORGET YOU CAN SCRIPT EVERYTHING,
SO EVEN THE RESTORE CAN HAVE ITS SCRIPT GENERATED AND YOU CAN USE THAT AS A BASE
TO BUILD ONE OF YOUR OWN AND ANALYZE IT)

=== Other system Databases (model, distribution)

Another set of databases that are critical for the functioning of the SQL Server.

==== Model System Database

- The `model` database is a sketch, a template of a database, and the idea is that
whenever we create a database in MSSQL Server, it will take the first information
about information from the `model` database. It's cool, _so that's how we get
an initial working database already operational even though it needs tons of
components and setup_.
- `tempdb` is created every time SQL Server is started, the `model` database must
always exist on a SQL Server system. So `model` is persistent, instead of the
`tempdb` that gets deleted on every restart
- Some of the settings of `model` are also used for creating a new `tempdb` during
start up, so the `model` database must always exist on a SQL Server system
- Newly created user databases will use the _same recovery model as the `model`
database_

**Usage**

- When a `CREATE DATABASE` statement is issued, the first part of the database is
created by copying the contents of the `model` database
- The rest of the new database is then filled with `empty pages`
- If you modify the `model` database, all databases created after said change will
**inherit** them.
	- E.g., You could set permissions, options, tables, functions, stored procedures
		- That way you have `pre-loaded` structures for every subsequent database

[cols="1,1,1,1"]
|===
|File | Logical Name | Physical Name | File Growth

|Primary data
|modeldev
|model.mdf
|Autogrow by 64 MB until the disk is full

|Log
|modellog
|modellog.ldf
|Autogrow by 64 MB to a maximum of 2 terabytes
|===

**Restrictions**

- We cannot add filegroups
- We cannot change the collation
- We cannot change the owner
- We cannot drop the `guest`
- We cannot enable data capture
- We cannot bring OFFLINE
- We cannot set it as READ_ONLY
- We cannot _create encrypted objects_, and even if somehow do, these objects
**will not be passed over to the other databases**

==== Resource System Database

- Is a read-only database that contains all the system objects that are included
with SQL Server
- `sys.objects` and all other objects similar to this, **physically are saved
to the `Resource System Database`. It's actually the primary repository
- However, on the `sys` schema of all databases, all these objects appear
**logically**
- The Resource database does not contain user data or user metadata
- The Resource database makes upgrading to a new version of SQL Server an easier
and faster procedure
	- That is because, we used to have to `DROP` objects, _recreate them_, and
	other stuff. But **now** because of the resource datafile, we can simply
	**copy the single resource database file to the local server**, _easy upgrade_

**Physical Properties of resource database**

Since this is _read-only_, the properties are indeed different

- The physical filename is `mssqlsystemresource,mdf` and `mssqlsystemresource.ldf`
- These files are located in _<drive>:\Program Files\Microsoft SQL Server\MSSQL<version>.<instance_name>\MSSQL\Binn\
and **should not be moved**. If you do mess around too much with this database,
**the instance will not start**, it's _really sensitive_
- Each instance of SQL Server has one and only associated `mssqlsystemresource.mdf`
file, and instances do not share this file. _Each instance will have its Resource
database_
- SQL Server cannot back up the Resource database (because as we mentioned before,
_it's hidden_)
	- **However**, we can indeed do backup _manually_, the file is `binary`,
	so you can simply backup the file at the filesystem level
- Backup is only possible manually

**Accessing the Resource Database**

This `Resource` database should not be modified unless a Specialist from Microsoft
comes in (which is like, never).

- The ID of the Resource database is always 32767
- Other important values are the version number and the last time the database
was updated

**Lab Notes**

- So there was a really good example of troubleshooting and understanding what
happens when we can't get a lock on a database perhaps.
	- This happened after adding a table to the `model` database, adding some
	records to it, and then trying to create a simple `Test` database. It was
	really strange, _however_ the course showed how we can use `sp_who2 active`
	to list all the active connections, and if we open a new query window, and
	try to run the `CREATE TABLE` and then on the other window run the stored
	procedure, we will see how a `SUSPENDED` connection shows up, and it also
	shows under a table an ID for a connection that is blocking it. Which in
	the course it was some weird connection that was there
		- In my case, it was strange, it prompted me that the session running the
		`sp` was blocking the create, _which makes no sense_.
		- Apparently `sp_who2` is not _that reliable_, so you can use a table that's
		on the system tables (remember this lives in the `Resource` database)

This is a better query that shows _"ghost connections"_ that we can properly kill:

[source, sql]
----
SELECT
	request_session_id AS [SessionID],
	DB_NAME(resource_database_id) AS [Database],
	request_mode AS [LockType],
	request_status AS [Status]
FROM sys.dm_tran_locks <.>
WHERE resource_database_id = DB_ID('model') <.>
----
<.> Special attention should be brought up here, whilst standing on **any database**,
we can query the `sys` databases, remember that all of them point to the same
`physical resource`
<.> And in here we are leveraging `DB_ID` and sending a database name to it so
we get the actual ID to match to the `resource_database_id`

It was **finally** with this query that I saw `2 connections` with a lock, the
one I used with `sp_who2` and another `54` connection. So, I killed it as that
session deserved it: `KILL 54`. After this, I could run the creation of a table
just fine:

````
USE master
GO
CREATE DATABASE Test
````

As a _safety measure_ I also made sure to switch to `master` that way, we are sure
the current window is not somehow still connected to `model`

_So after all this mess_.

If we refresh the instance, we should see the new database, and if we expand its
tables, **we should also see the table that we created on the model database already
there, and with the same data**. This is a `prototype` pattern. COOL

And so if we head down to:

`Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\Binn\`

We should be able to visualize `mssqlsystemresource.ldf` and `mssqlsystemresource.mdf`,
which are the logs and data files respectively.

- If you go into the `Views > System Views` of the freshly created database,
we would be able to see all sorts of `sys.` views on top of some other extra views,
they are there already set for you to use. But remember **NONE OF THIS IS ACTUALLY
COPIED OVER TO THE NEWLY CREATED DATABASE**, all of these views are being piped
from the same path that is the `Resource` database.

**Some extra useful functions**

- `SELECT SERVERPROPERTY('ResourceVersion')` = Another way to checkout the SQL
Server version for the resource database
- `SELECT SERVERPROPERTY('ResourceLastUpdateDateTime')` = Another way to check out
in long date format **when** was the resource database updated in any capacity
- `SELECT OBJECT_DEFINITION(OBJECT_ID('sys.objects'))` = This is a way to retrieve
an SQL query of how the specific `sys.objects` objects got into the current database

_EXTRA_

There are two extra databases: Distribution, Report Server. These are used for
replication or SSRS respectively. _We will discussing them later_.

=== Quiz 6: Microsoft SQL Database Administration and System databases
